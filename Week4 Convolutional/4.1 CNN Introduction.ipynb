{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rnoXYOlfjSvb"
   },
   "source": [
    "# Convolutional neural networks 101\n",
    "\n",
    "Convolution neural networks are one of the most successful types of neural networks for image recognition and an integral part of reigniting the interest in neural networks. They are able to extract structural relations in the data, such as spatial in images or temporal in time series.\n",
    "\n",
    "In this lab, we will experiment with inserting 2D-convolution layers in the fully connected neural networks introduced previously. We will also try to visualize the learned convolution filters and try to understand what kind of features they learn to recognize.\n",
    "\n",
    "If you have not watched Jason Yosinski's [video on visualizing convolutional networks](https://www.youtube.com/watch?v=AgkfIQ4IGaM), you definitely should do so now. If you are unfamiliar with the convolution operation, [Vincent Dumoulin](https://github.com/vdumoulin/conv_arithmetic) has a nice visualization of different convolution variants. For a more in-depth tutorial, please see also [cs231n.github.io/convolutional-networks](http://cs231n.github.io/convolutional-networks)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2lMa9SSfjSvl"
   },
   "source": [
    "## Reminder: what are convolutional networks?\n",
    "\n",
    "Standard ConvNets are, in many respects, very similar to the dense feedforward networks we saw previously:\n",
    " * The network is still organized into layers.\n",
    " * Each layer is parameterized by weights and biases.\n",
    " * Each layer has an element-wise non-linear transformation (activation function).\n",
    " * There are no cycles in the connections (more on this in later labs).\n",
    "\n",
    "*So what is the difference?*\n",
    "The networks we saw previously are called *dense* because each unit receives input from all the units in the previous layer. This is not the case for ConvNets. In ConvNets each unit is only connected to a small subset of the input units. This is called the *receptive field* of the unit.\n",
    "\n",
    "#### Example\n",
    "The input (green matrix) is a tensor of size `1x5x5`, i.e., it has one \"channel\" (like a grayscale image), and the feature map has size `5x5`. Let us define a `1x3x3` kernel (yellow submatrix). The kernel weights are indicated in red at the bottom right of each element. The computation can be thought of as an elementwise multiplication followed by a sum. Here we use a *stride* of 1, as shown in this animation:\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/DeepLearningDTU/02456-deep-learning-with-PyTorch/master/4_Convolutional/images/convolutions.gif\" style=\"width: 400px;\"/>\n",
    "\n",
    "(GIF courtesy of [Stanford](http://deeplearning.stanford.edu/wiki/index.php/Feature_extraction_using_convolution))\n",
    "\n",
    "After having convolved the image, we perform an elementwise non-linear transformation on the *convolved features*.\n",
    "In this example, the input is a 2D *feature map* with depth 1.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pOLUjfUZjSvn"
   },
   "source": [
    "# Assignment 1\n",
    "\n",
    "### Assignment 1.1: Manual calculations\n",
    "\n",
    "Perform the following computation, and write the result below.\n",
    "\n",
    "![](https://raw.githubusercontent.com/DeepLearningDTU/02456-deep-learning-with-PyTorch/master/4_Convolutional/images/conv_exe.png)\n",
    "\n",
    "1. Manually convolve the input, and compute the convolved features. No padding and stride of 1.\n",
    " * **Answer:** With a 5×5 input and 3×3 kernel (stride=1, padding=0), the output is 3×3. Each element is computed as the sum of elementwise products between the kernel and each 3×3 patch of the input.\n",
    "2. Perform `2x2` max pooling on the convolved features. Stride of 2.\n",
    " * **Answer:** Applying 2×2 max pooling with stride=2 on the 3×3 convolved features yields a 1×1 output (since $\\lfloor(3-2)/2\\rfloor + 1 = 1$).\n",
    "\n",
    "### Assignment 1.2: Output dimensionality\n",
    "\n",
    "Given the following 3D tensor input `(channel, height, width)`, a given amount (`channels_out`) of filters `(channels_in, filter_height, filter_width)`, stride `(height, width)` and padding `(height, width)`, calculate the output dimensionality if it is valid.\n",
    "\n",
    "**Formula:** $H_{out} = \\lfloor\\frac{H + 2 \\cdot pad_h - K_h}{stride_h}\\rfloor + 1$ (same for width)\n",
    "\n",
    "1. input tensor with dimensionality (1, 28, 28) and 16 filters of size (1, 5, 5) with stride (1, 1) and padding (0, 0)\n",
    " * **Answer:** $(16, 24, 24)$ where $24 = \\lfloor(28-5)/1\\rfloor + 1$\n",
    "2. input tensor with dimensionality (2, 32, 32) and 24 filters of size (2, 3, 3) with stride (1, 1) and padding (0, 0)\n",
    " * **Answer:** $(24, 30, 30)$ where $30 = \\lfloor(32-3)/1\\rfloor + 1$\n",
    "3. input tensor with dimensionality (10, 32, 32) and 3 filters of size (10, 2, 2) with stride (2, 2) and padding (0, 0)\n",
    " * **Answer:** $(3, 16, 16)$ where $16 = \\lfloor(32-2)/2\\rfloor + 1$\n",
    "4. input tensor with dimensionality (11, 8, 16) and 7 filters of size (11, 3, 3) with stride (2, 2) and padding (1, 1)\n",
    " * **Answer:** $(7, 4, 8)$ where $H_{out} = \\lfloor(8+2-3)/2\\rfloor + 1 = 4$ and $W_{out} = \\lfloor(16+2-3)/2\\rfloor + 1 = 8$\n",
    "5. input tensor with dimensionality (128, 256, 256) and 112 filters of size (128, 3, 3) with stride (1, 1) and padding (1, 1)\n",
    " * **Answer:** $(112, 256, 256)$ where $256 = \\lfloor(256+2-3)/1\\rfloor + 1$\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fTsZa_xhjSvp"
   },
   "source": [
    "# Load packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 3086,
     "status": "ok",
     "timestamp": 1662639635584,
     "user": {
      "displayName": "Ole Winther",
      "userId": "12097569893493878263"
     },
     "user_tz": -120
    },
    "id": "oFUsDSEtjSvq"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "sns.set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TpD2ZUGWjSvs"
   },
   "source": [
    "# Load MNIST data\n",
    "\n",
    "The code below downloads and loads the same MNIST dataset as before.\n",
    "Note however that the data has a different shape this time: `(num_samples, num_channels, height, width)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "\n",
    "# Load the MNIST dataset\n",
    "train_dataset_full = datasets.MNIST(root=\"./data\", train=True, download=True, transform=transforms.ToTensor())\n",
    "test_dataset = datasets.MNIST(root=\"./data\", train=False, download=True, transform=transforms.ToTensor())\n",
    "\n",
    "# Create subsets: 50,000 for training, 10,000 for validation/testing\n",
    "train_dataset = Subset(train_dataset_full, range(50000))\n",
    "validation_dataset = Subset(train_dataset_full, range(50000, 60000))\n",
    "\n",
    "# Get the number of classes\n",
    "num_classes = train_dataset.dataset.targets.unique().size(0)\n",
    "\n",
    "# Extract a batch from each dataset and print its shape\n",
    "x_train, targets_train = next(iter(DataLoader(train_dataset, batch_size=64)))\n",
    "x_valid, targets_valid = next(iter(DataLoader(validation_dataset, batch_size=64)))\n",
    "x_test, targets_test = next(iter(DataLoader(test_dataset, batch_size=64)))\n",
    "\n",
    "print(\"Information on dataset\")\n",
    "print(\"Shape of x_train:\", x_train.shape)\n",
    "print(\"Shape of targets_train:\", targets_train.shape)\n",
    "print(\"Shape of x_valid:\", x_valid.shape)\n",
    "print(\"Shape of targets_valid:\", targets_valid.shape)\n",
    "print(\"Shape of x_test:\", x_test.shape)\n",
    "print(\"Shape of targets_test:\", targets_test.shape)\n",
    "print(\"Number of classes:\", num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the 100 images from the training set\n",
    "images, labels = next(iter(DataLoader(train_dataset, batch_size=100, shuffle=True)))\n",
    "\n",
    "# Plot a few MNIST examples\n",
    "plt.figure(figsize=(7, 7))\n",
    "plt.imshow(make_grid(images, nrow=10).permute(1, 2, 0))\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xcq0TsjbjSv1"
   },
   "source": [
    "# Define a simple feed forward neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 312,
     "status": "ok",
     "timestamp": 1662639671015,
     "user": {
      "displayName": "Ole Winther",
      "userId": "12097569893493878263"
     },
     "user_tz": -120
    },
    "id": "_Cb9FsjajSv2",
    "outputId": "7691e76d-3420-436f-e0c6-9072cdf925f4"
   },
   "outputs": [],
   "source": [
    "channels, height, width = x_train.shape[1:]\n",
    "n_features = channels * height * width\n",
    "\n",
    "\n",
    "class PrintSize(nn.Module):\n",
    "    \"\"\"Utility module to print current shape of a Tensor in Sequential, only at the first forward pass.\"\"\"\n",
    "    \n",
    "    first = True\n",
    "    \n",
    "    def forward(self, x):\n",
    "        if self.first:\n",
    "            print(f\"Size: {x.size()}\")\n",
    "            self.first = False\n",
    "        return x\n",
    "\n",
    "\n",
    "class Model(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        activation_fn = nn.ReLU\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Flatten(),  # from (1, channels, height, width) to (1, channels * height * width)\n",
    "            nn.Linear(n_features, 128),\n",
    "            activation_fn(),\n",
    "            nn.Linear(128, 128),\n",
    "            activation_fn(),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "# ConvNet variants for Assignment 2\n",
    "class ConvModel1(nn.Module):\n",
    "    \"\"\"Single Conv2d before the MLP. Try num_filters=32, kernel_size=5, padding=2 to preserve spatial size.\"\"\"\n",
    "    def __init__(self, num_filters=32, kernel_size=5, padding=2):\n",
    "        super().__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            PrintSize(),\n",
    "            nn.Conv2d(channels, num_filters, kernel_size=kernel_size, stride=1, padding=padding),\n",
    "            nn.ReLU(),\n",
    "            PrintSize(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),  # optional for part 4\n",
    "            PrintSize(),\n",
    "        )\n",
    "        # After pooling, H,W halved if MaxPool2d applied\n",
    "        h_out = height // 2\n",
    "        w_out = width // 2\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(num_filters * h_out * w_out, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        return self.classifier(x)\n",
    "\n",
    "\n",
    "class ConvModel2(nn.Module):\n",
    "    \"\"\"Stack two Conv2d layers to test improved performance.\"\"\"\n",
    "    def __init__(self, f1=32, f2=64, k1=5, k2=3, p1=2, p2=1):\n",
    "        super().__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            PrintSize(),\n",
    "            nn.Conv2d(channels, f1, kernel_size=k1, stride=1, padding=p1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(f1, f2, kernel_size=k2, stride=1, padding=p2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2,2),\n",
    "            PrintSize(),\n",
    "        )\n",
    "        h_out = height // 2\n",
    "        w_out = width // 2\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(f2 * h_out * w_out, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        return self.classifier(x)\n",
    "\n",
    "\n",
    "class ConvModelReg(nn.Module):\n",
    "    \"\"\"ConvNet with Dropout and BatchNorm to test regularization effects.\"\"\"\n",
    "    def __init__(self, f1=32, f2=64, dropout_p=0.3):\n",
    "        super().__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(channels, f1, kernel_size=5, padding=2),\n",
    "            nn.BatchNorm2d(f1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(f1, f2, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(f2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2,2),\n",
    "        )\n",
    "        h_out = height // 2\n",
    "        w_out = width // 2\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Dropout(p=dropout_p),\n",
    "            nn.Linear(f2 * h_out * w_out, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=dropout_p),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        return self.classifier(x)\n",
    "\n",
    "\n",
    "# Choose baseline (feedforward) for comparison and a conv model\n",
    "model = Model()\n",
    "print(model)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 313,
     "status": "ok",
     "timestamp": 1662639684841,
     "user": {
      "displayName": "Ole Winther",
      "userId": "12097569893493878263"
     },
     "user_tz": -120
    },
    "id": "GPIgjOTFjSv6",
    "outputId": "7401463e-3427-4a1a-ec03-620dc002d312"
   },
   "outputs": [],
   "source": [
    "# Test the forward pass with dummy data on different models\n",
    "print(\"Baseline MLP:\")\n",
    "out = model(torch.randn(2, 1, 28, 28))\n",
    "print(\"Output shape:\", out.size())\n",
    "print(f\"Sample probabilities:\\n{out.softmax(1).detach().numpy()}\")\n",
    "\n",
    "print(\"\\nConvModel1 (1 conv + pool):\")\n",
    "model_c1 = ConvModel1(num_filters=32, kernel_size=5, padding=2)\n",
    "_ = model_c1(torch.randn(2, 1, 28, 28))\n",
    "\n",
    "print(\"\\nConvModel2 (2 convs + pool):\")\n",
    "model_c2 = ConvModel2(f1=32, f2=64)\n",
    "_ = model_c2(torch.randn(2, 1, 28, 28))\n",
    "\n",
    "print(\"\\nConvModelReg (BN + Dropout):\")\n",
    "model_reg = ConvModelReg(f1=32, f2=64, dropout_p=0.3)\n",
    "_ = model_reg(torch.randn(2, 1, 28, 28))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p5kIR7JTjSv7"
   },
   "source": [
    "# Train network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2eBO7B36jSv7"
   },
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "num_epochs = 5\n",
    "validation_every_steps = 500\n",
    "\n",
    "# Make data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "validation_loader = DataLoader(validation_dataset, batch_size=batch_size, shuffle=False, drop_last=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, drop_last=False)\n",
    "\n",
    "step = 0\n",
    "model.train()\n",
    "\n",
    "train_accuracies = []\n",
    "validation_accuracies = []\n",
    "        \n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    train_accuracies_batches = []\n",
    "    \n",
    "    for inputs, targets in train_loader:\n",
    "        \n",
    "        # Forward pass.\n",
    "        output = model(inputs)\n",
    "        \n",
    "        # Compute loss.\n",
    "        loss = loss_fn(output, targets)\n",
    "        \n",
    "        # Clean up gradients from the model.\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Compute gradients based on the loss from the current batch (backpropagation).\n",
    "        loss.backward()\n",
    "        \n",
    "        # Take one optimizer step using the gradients computed in the previous step.\n",
    "        optimizer.step()\n",
    "        \n",
    "        step += 1\n",
    "        \n",
    "        # Compute accuracy.\n",
    "        predictions = output.max(1)[1]\n",
    "        train_accuracies_batches.append(accuracy_score(targets, predictions))\n",
    "        \n",
    "        if step % validation_every_steps == 0:\n",
    "            \n",
    "            # Append average training accuracy to list.\n",
    "            train_accuracies.append(np.mean(train_accuracies_batches))\n",
    "            \n",
    "            train_accuracies_batches = []\n",
    "        \n",
    "            # Compute accuracies on validation set.\n",
    "            validation_accuracies_batches = []\n",
    "            with torch.no_grad():\n",
    "                model.eval()\n",
    "                for inputs, targets in validation_loader:\n",
    "                    output = model(inputs)\n",
    "                    loss = loss_fn(output, targets)\n",
    "\n",
    "                    predictions = output.max(1)[1]\n",
    "\n",
    "                    # Multiply by len(x) because the final batch of DataLoader may be smaller (drop_last=False).\n",
    "                    validation_accuracies_batches.append(accuracy_score(targets, predictions) * len(inputs))\n",
    "\n",
    "                model.train()\n",
    "                \n",
    "            # Append average validation accuracy to list.\n",
    "            validation_accuracies.append(np.sum(validation_accuracies_batches) / len(validation_dataset))\n",
    "     \n",
    "            print(f\"Step {step:<5}   training accuracy: {train_accuracies[-1]}\")\n",
    "            print(f\"             validation accuracy: {validation_accuracies[-1]}\")\n",
    "\n",
    "print(\"Finished training.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "giiAmxpYjSv-"
   },
   "outputs": [],
   "source": [
    "steps = (np.arange(len(train_accuracies), dtype=int) + 1) * validation_every_steps\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(steps, train_accuracies, label='train')\n",
    "plt.plot(steps, validation_accuracies, label='validation')\n",
    "plt.xlabel('Training steps')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.title(\"Train and validation accuracy\")\n",
    "plt.show()\n",
    "\n",
    "# Evaluate test set\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    test_accuracies = []\n",
    "    for inputs, targets in test_loader:\n",
    "        output = model(inputs)\n",
    "        loss = loss_fn(output, targets)\n",
    "\n",
    "        predictions = output.max(1)[1]\n",
    "\n",
    "        # Multiply by len(x) because the final batch of DataLoader may be smaller (drop_last=True).\n",
    "        test_accuracies.append(accuracy_score(targets, predictions) * len(inputs))\n",
    "\n",
    "    test_accuracy = np.sum(test_accuracies) / len(test_dataset)\n",
    "    print(f\"Validation accuracy: {validation_accuracies[-1]:.3f}\")\n",
    "    print(f\"Test accuracy: {test_accuracy:.3f}\")\n",
    "    \n",
    "    model.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0l4hf_-ijSv-"
   },
   "source": [
    "### Assignment 2\n",
    "\n",
    "1. Note the performance of the standard feedforward neural network. Add a [2D convolution layer](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html) before the first layer. Insert the utility module `PrintSize` to check the size of the tensor at any point in `Sequential`, and notice that the size of the image reduces after the convolution. This can cause loss of information, and can be avoided by using adequate padding in the convolutional layer.\n",
    "  Does adding a convolutional layer increase the generalization performance of the network (try num_filters=32 and filter_size=5 as a starting point)?\n",
    "  \n",
    "2. Can the performance be increases even further by stacking more convolution layers?\n",
    "\n",
    "3. We now have a deeper network than the initial simple feedforward network. What happens if we replace all convolutional layers with linear layers? Is this deep feedforward network performing as well as the convolutional one?\n",
    " \n",
    "4. Max-pooling is a technique for decreasing the spatial resolution of an image while retaining the important features. Effectively this gives a local translational invariance and reduces the computation by a factor of four. In the classification algorithm which is usually desirable. You can either: \n",
    " \n",
    "   - add a maxpool layer (see the PyTorch docs, and try with kernel_size=2 and stride=2) after the convolution layer, or\n",
    "   - add stride=2 to the arguments of the convolution layer directly.\n",
    "     \n",
    "  Verify that this decreases the spatial dimension of the image (insert a `PrintSize` module in the `Sequential`). Does this increase the performance of the network? Note that, to increase performance, you may need to stack multiple layers, increase the number of filters, or tune the learning rate.\n",
    "\n",
    "5. Dropout is a very useful technique for preventing overfitting. Try to add a DropoutLayer after some of the convolution layers. You may observe a higher validation accuracy but lower train accuracy. Can you explain why this might be the case?\n",
    " \n",
    "6. Batch normalization may help convergence in larger networks as well as generalization performance. Try to insert batch normalization layers into the network.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solutions: Assignment 2\n",
    "\n",
    "**1. Adding a Conv2d layer:**\n",
    "- A 2D convolution layer extracts spatial features before the MLP. With `padding=2` for a 5×5 kernel, spatial size is preserved (28×28 → 28×28).\n",
    "- Without padding, size reduces (28×28 → 24×24 for kernel_size=5), which may lose border information.\n",
    "- **Implementation:** See `ConvModel1` class above with `PrintSize` to track tensor shapes.\n",
    "- **Performance:** Conv layers typically improve generalization vs. pure MLP by leveraging spatial structure.\n",
    "\n",
    "**2. Stacking multiple conv layers:**\n",
    "- Deeper conv architectures can learn hierarchical features (edges → textures → parts → objects).\n",
    "- **Implementation:** See `ConvModel2` class with two conv layers (32 → 64 filters).\n",
    "- **Note:** May require tuning learning rate and more training epochs.\n",
    "\n",
    "**3. Replacing conv with linear layers:**\n",
    "- A deep feedforward network with only linear layers lacks spatial inductive bias.\n",
    "- Performance typically degrades compared to conv networks on image data due to:\n",
    "  - No parameter sharing across spatial locations\n",
    "  - No translation equivariance\n",
    "  - Vastly more parameters (harder to train, prone to overfitting)\n",
    "\n",
    "**4. Max pooling:**\n",
    "- Max pooling with `kernel_size=2, stride=2` reduces spatial dimensions by half (28×28 → 14×14).\n",
    "- Benefits: \n",
    "  - Reduces computation (fewer parameters in subsequent layers)\n",
    "  - Provides local translation invariance\n",
    "  - Prevents overfitting by downsampling\n",
    "- **Verification:** `PrintSize` modules in `ConvModel1` and `ConvModel2` show dimension reduction.\n",
    "- Alternative: Use `stride=2` directly in Conv2d instead of separate MaxPool layer.\n",
    "\n",
    "**5. Dropout:**\n",
    "- Dropout randomly zeros activations during training, preventing co-adaptation of neurons.\n",
    "- **Expected behavior:** \n",
    "  - Training accuracy may be lower (model can't rely on all features)\n",
    "  - Validation accuracy may be higher (better generalization)\n",
    "- **Explanation:** Dropout acts as regularization, trading training performance for test performance.\n",
    "- **Implementation:** See `ConvModelReg` class with `Dropout(p=0.3)` in the classifier.\n",
    "\n",
    "**6. Batch Normalization:**\n",
    "- BatchNorm normalizes layer inputs, stabilizing and accelerating training.\n",
    "- Benefits:\n",
    "  - Allows higher learning rates\n",
    "  - Reduces sensitivity to initialization\n",
    "  - Acts as regularization (slight performance gain on validation)\n",
    "- **Implementation:** See `ConvModelReg` class with `BatchNorm2d` after each conv layer.\n",
    "\n",
    "**All model classes are implemented above. To train a specific variant, replace `model = Model()` with:**\n",
    "```python\n",
    "# model = ConvModel1(num_filters=32, kernel_size=5, padding=2)\n",
    "# model = ConvModel2(f1=32, f2=64)\n",
    "# model = ConvModelReg(f1=32, f2=64, dropout_p=0.3)\n",
    "# optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "```\n",
    "Then run the training loop."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nxnfgk0JjSwA"
   },
   "source": [
    "Again, if you didn't already, you really should [watch this video](https://www.youtube.com/watch?v=AgkfIQ4IGaM)."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
