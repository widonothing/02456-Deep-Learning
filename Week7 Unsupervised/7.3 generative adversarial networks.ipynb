{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-danger\">\n",
    "It is recommended to run this notebook on a GPU\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Image, display, clear_output\n",
    "import numpy as np\n",
    "%matplotlib nbagg\n",
    "%matplotlib inline\n",
    "plt.style.use([\"seaborn-deep\", \"seaborn-whitegrid\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generative Adversarial Networks\n",
    "\n",
    "Generative Adversarial Networks (GAN) [[Goodfellow, 2014]](https://arxiv.org/abs/1406.2661) have recently become a popular alternative to variational autoencoders for generative modelling and to a lesser extend semi-supervised learning. They also represent the state-of-the-art in modelling of realistic images and video just four years after their introduction. Below you can see a comparison of the development in GANs for generation of realistic faces from 2014 until today.\n",
    "\n",
    "<img src=\"https://github.com/DeepLearningDTU/02456-2025/blob/master/static_files/GAN-celebA.jpg?raw=1\" alt=\"GAN performance over the years\" width=\"600px\"/>\n",
    "\n",
    "Different variants of GANs have also proven to perform well on tasks such inpainting, super-resolution and image-to-image translation. In this notebook we will again work with a subset of the MNIST-dataset in order to compare with VAEs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if cuda else \"cpu\")\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.transforms import ToTensor\n",
    "from functools import reduce\n",
    "\n",
    "# The digit classes to use, these need to be in order because\n",
    "# we are using one-hot representation\n",
    "classes = np.arange(2)\n",
    "\n",
    "def one_hot(labels):\n",
    "    y = torch.eye(len(classes)) \n",
    "    return y[labels]\n",
    "\n",
    "# Define the train and test sets\n",
    "dset_train = MNIST(\"./\", train=True, download=True, transform=ToTensor(), target_transform=one_hot)\n",
    "dset_test  = MNIST(\"./\", train=False, transform=ToTensor(), target_transform=one_hot)\n",
    "\n",
    "def stratified_sampler(labels):\n",
    "    \"\"\"Sampler that only picks datapoints corresponding to the specified classes\"\"\"\n",
    "    (indices,) = np.where(reduce(lambda x, y: x | y, [labels.numpy() == i for i in classes]))\n",
    "    indices = torch.from_numpy(indices)\n",
    "    return SubsetRandomSampler(indices)\n",
    "\n",
    "\n",
    "batch_size = 64\n",
    "# The loaders perform the actual work\n",
    "train_loader = DataLoader(dset_train, batch_size=batch_size,\n",
    "                          sampler=stratified_sampler(dset_train.train_labels), pin_memory=cuda)\n",
    "test_loader  = DataLoader(dset_test, batch_size=batch_size, \n",
    "                          sampler=stratified_sampler(dset_test.test_labels), pin_memory=cuda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adversarial learning\n",
    "\n",
    "The training process of a GAN can be seen as a two player game involving a discriminator network ($D$) and a generator network($G$). Intuitively, we can describe the role of the two networks as \"police\" and \"forger\", respectively. Given some empirical distribution $p(x)$, the forger wants to fool the police by creating samples that look like they come from $p(x)$. The police will then try to \"analayse each art piece\" to guess whether it is forged or not. This process leads the generator to eventually generate samples that are indistinguishable from the real data.\n",
    "\n",
    "<img src=\"https://github.com/DeepLearningDTU/02456-2025/blob/master/static_files/GAN.png?raw=1\" alt=\"GAN diagram\" width=\"500px\"/>\n",
    "\n",
    "Below we define a deep convolutional generative adversarial network (DCGAN), introduced by [[Radford, 2015]](https://arxiv.org/abs/1511.06434). This means that both the discriminator and generator are deep convolutional networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "latent_dim = 100\n",
    "\n",
    "# The generator takes random `latent` noise and\n",
    "# turns it into an MNIST image.\n",
    "generator = nn.Sequential(\n",
    "    # nn.ConvTranspose2d can be seen as the inverse operation\n",
    "    # of Conv2d, where after convolution we arrive at an\n",
    "    # upscaled image.\n",
    "    nn.ConvTranspose2d(latent_dim, 256, kernel_size=3, stride=2),\n",
    "    nn.BatchNorm2d(256),\n",
    "    nn.ReLU(),\n",
    "    nn.ConvTranspose2d(256, 128, kernel_size=3, stride=2),\n",
    "    nn.BatchNorm2d(128),\n",
    "    nn.ReLU(),\n",
    "    nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2),\n",
    "    nn.BatchNorm2d(64),\n",
    "    nn.ReLU(),\n",
    "    nn.ConvTranspose2d(64, 1, kernel_size=2, stride=2),\n",
    "    nn.Sigmoid() # Image intensities are in [0, 1]\n",
    ").to(device)\n",
    "\n",
    "class Flatten(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return x.view(x.size(0), -1)\n",
    "\n",
    "# The discriminator takes an image (real or fake)\n",
    "# and decides whether it is generated or not.\n",
    "discriminator = nn.Sequential(\n",
    "    nn.Conv2d(1, 64, kernel_size=4, stride=2),\n",
    "    nn.LeakyReLU(0.2),\n",
    "    nn.Conv2d(64, 128, kernel_size=4, stride=2),\n",
    "    nn.BatchNorm2d(128),\n",
    "    nn.LeakyReLU(0.2),\n",
    "    nn.Conv2d(128, 256, kernel_size=4, stride=2),\n",
    "    nn.BatchNorm2d(256),\n",
    "    nn.LeakyReLU(0.2),\n",
    "    Flatten(),\n",
    "    nn.Linear(256, 1),\n",
    "    nn.Sigmoid()\n",
    ").to(device)\n",
    "\n",
    "loss = nn.BCELoss()\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "generator_optim = torch.optim.Adam(generator.parameters(), 2e-4, betas=(0.5, 0.999))\n",
    "discriminator_optim = torch.optim.Adam(discriminator.parameters(), 2e-4, betas=(0.5, 0.999))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The GAN game\n",
    "\n",
    "The objective function can be formulated within the framework of game-theory, concretely as a zero-sum game. The optimum is therefore given by the *Nash equilibrium* between $D$ and $G$. Unfortunately, there exists no such algorithm capable of finding the Nash equilibrium directly, so we must instead resort to gradient descent, for which we arrive at the following objective function $V(D, G)$.\n",
    "\n",
    "$$\\min_{G}\\max_{D} V(D, G) = \\mathbb{E}_{x \\sim p(x)} [\\log D(x)] + \\mathbb{E}_{z \\sim p(z)} [\\log(1 - D(G(z)))]$$\n",
    "\n",
    "Where $x \\sim p(x)$ is sampled from the true distribution and $z \\sim p(z)$ is a sample from the noise distribution. To break down this objective we consider the first term $\\max_{D}\\mathbb{E}_{x \\sim p(x)} [\\log D(x)]$, which is the log-likelihood of the discriminator correctly classifying a data point as coming from the true distribution. The second term $\\min_{G}\\max_{D} \\mathbb{E}_{z \\sim p(z)} [\\log(1 - D(G(z)))]$ can be seen as a dual objective of the discriminator correctly rejecting a sample from the generator by maximising the likelihood, while simultaneously, the generator should minimise the chance of being \"caught\" by the discriminator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "import os\n",
    "\n",
    "tmp_img = \"tmp_gan_out.png\"\n",
    "discriminator_loss, generator_loss = [], []\n",
    "\n",
    "num_epochs = 50\n",
    "for epoch in range(num_epochs):\n",
    "    batch_d_loss, batch_g_loss = [], []\n",
    "    \n",
    "    for x, _ in train_loader:\n",
    "        batch_size = x.size(0)\n",
    "        # True data is given label 1, while fake data is given label 0\n",
    "        true_label = torch.ones(batch_size, 1).to(device)\n",
    "        fake_label = torch.zeros(batch_size, 1).to(device)\n",
    "        \n",
    "        discriminator.zero_grad()\n",
    "        generator.zero_grad()\n",
    "        \n",
    "        # Step 1. Send real data through discriminator\n",
    "        #         and backpropagate its errors.\n",
    "        x_true = Variable(x).to(device)        \n",
    "        output = discriminator(x_true)\n",
    "        \n",
    "        error_true = loss(output, true_label)\n",
    "        error_true.backward()\n",
    "        \n",
    "        # Step 2. Generate fake data G(z), where z ~ N(0, 1)\n",
    "        #         is a latent code.\n",
    "        z = torch.randn(batch_size, latent_dim, 1, 1)\n",
    "        z = Variable(z, requires_grad=False).to(device)\n",
    "        \n",
    "        x_fake = generator(z)\n",
    "            \n",
    "        # Step 3. Send fake data through discriminator\n",
    "        #         propagate error and update D weights.\n",
    "        # --------------------------------------------\n",
    "        # Note: detach() is used to avoid compounding generator gradients\n",
    "        output = discriminator(x_fake.detach()) \n",
    "        \n",
    "        error_fake = loss(output, fake_label)\n",
    "        error_fake.backward()\n",
    "        discriminator_optim.step()\n",
    "        \n",
    "        # Step 4. Send fake data through discriminator _again_\n",
    "        #         propagate the error of the generator and\n",
    "        #         update G weights.\n",
    "        output = discriminator(x_fake)\n",
    "        \n",
    "        error_generator = loss(output, true_label)\n",
    "        error_generator.backward()\n",
    "        generator_optim.step()\n",
    "        \n",
    "        batch_d_loss.append((error_true/(error_true + error_fake)).item())\n",
    "        batch_g_loss.append(error_generator.item())\n",
    "\n",
    "    discriminator_loss.append(np.mean(batch_d_loss))\n",
    "    generator_loss.append(np.mean(batch_g_loss))\n",
    "    \n",
    "    # -- Plotting --\n",
    "    f, axarr = plt.subplots(1, 2, figsize=(18, 7))\n",
    "\n",
    "    # Loss\n",
    "    ax = axarr[0]\n",
    "    ax.set_xlabel('Epoch')\n",
    "    ax.set_ylabel('Loss')\n",
    "\n",
    "    ax.plot(np.arange(epoch+1), discriminator_loss)\n",
    "    ax.plot(np.arange(epoch+1), generator_loss, linestyle=\"--\")\n",
    "    ax.legend(['Discriminator', 'Generator'])\n",
    "    \n",
    "    # Latent space samples\n",
    "    ax = axarr[1]\n",
    "    ax.set_title('Samples from generator')\n",
    "    ax.axis('off')\n",
    "\n",
    "    rows, columns = 8, 8\n",
    "    \n",
    "    # Generate data\n",
    "    with torch.no_grad():\n",
    "        z = torch.randn(rows*columns, latent_dim, 1, 1)\n",
    "        z = Variable(z, requires_grad=False).to(device)\n",
    "        x_fake = generator(z)\n",
    "    \n",
    "    canvas = np.zeros((28*rows, columns*28))\n",
    "    for i in range(rows):\n",
    "        for j in range(columns):\n",
    "            idx = i % columns + rows * j\n",
    "            canvas[i*28:(i+1)*28, j*28:(j+1)*28] = x_fake.data[idx]\n",
    "    ax.imshow(canvas, cmap='gray')\n",
    "    \n",
    "    plt.savefig(tmp_img)\n",
    "    plt.close(f)\n",
    "    display(Image(filename=tmp_img))\n",
    "    clear_output(wait=True)\n",
    "\n",
    "    os.remove(tmp_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises\n",
    "\n",
    "\n",
    "## Exercise 1: Analyzing the GAN\n",
    "* Reduce the latent space dimension, is the generator still able to create convincing samples? Give an explanation for what you see (*hint: think of the generator as the inverse of a non-linear PCA*).\n",
    "* Try training the GAN a couple of times using different digits and latent space dimension; does training always converge? If it doesn't, what happens?\n",
    "* Consider the case when the generator is perfect, effectively meaning that any sample from $G(z)$ is indistinguishable from a sample from the true distribution. What is then the value $D(x)$ for any $x$? Is this value an optimum?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution\n",
    "\n",
    "### Question 1: Reducing the latent space dimension\n",
    "\n",
    "**Observation**: When reducing the latent space dimension (e.g., from 100 to 10 or even 2), the generator can still create recognizable samples, but with some limitations:\n",
    "\n",
    "- **Small latent dimensions (e.g., 2-10)**: The generator can still produce digits that look like 0s and 1s, but the diversity of samples decreases. Many generated images may look similar, and fine details may be lost.\n",
    "\n",
    "- **Very small dimensions (e.g., 1-2)**: The generator struggles to capture the full variability of the data distribution. Samples become very similar or blurry.\n",
    "\n",
    "**Explanation**: Think of the generator as performing inverse non-linear PCA:\n",
    "- The latent code $z$ represents a compressed representation of the data in a low-dimensional manifold\n",
    "- PCA finds a linear subspace that captures maximum variance; the generator learns a non-linear manifold\n",
    "- When the latent dimension is too small, this manifold cannot capture all the variance and complexity of the true data distribution\n",
    "- Just as PCA with too few components loses information, a GAN with too small latent dimension cannot represent the full diversity of the dataset\n",
    "- However, unlike PCA, the non-linear nature of neural networks allows GANs to be more efficient with dimensions\n",
    "\n",
    "### Question 2: Convergence behavior\n",
    "\n",
    "**Observations from training with different settings**:\n",
    "\n",
    "1. **Convergence is not guaranteed**: GAN training is notoriously unstable. You may observe:\n",
    "   - **Mode collapse**: The generator produces only a few types of samples (e.g., only 0s or only 1s), ignoring part of the data distribution\n",
    "   - **Oscillation**: Discriminator and generator losses oscillate without converging\n",
    "   - **Discriminator dominance**: If the discriminator becomes too strong too quickly, gradients to the generator vanish, and learning stops\n",
    "   - **Generator dominance**: If the generator fools the discriminator completely early on, the discriminator provides no useful gradient signal\n",
    "\n",
    "2. **Different digits**: Some digits are easier to generate than others (e.g., 1s are simpler than 8s)\n",
    "\n",
    "3. **Latent dimension effects**: Very small dimensions may prevent convergence due to insufficient capacity; very large dimensions may slow training\n",
    "\n",
    "**What happens when training fails**:\n",
    "- Generator produces nonsensical or constant outputs\n",
    "- Loss curves show one network dominating (discriminator loss near 0 or 1, generator loss exploding)\n",
    "- Generated samples don't improve or degrade over epochs\n",
    "\n",
    "### Question 3: Perfect generator and optimal discriminator value\n",
    "\n",
    "**Analysis**:\n",
    "\n",
    "If the generator is perfect, then $p_G(x) = p_{data}(x)$ for all $x$. In this case:\n",
    "\n",
    "The optimal discriminator value is $D^*(x) = \\frac{1}{2}$ for any $x$.\n",
    "\n",
    "**Proof**:\n",
    "From the GAN objective, the discriminator aims to maximize:\n",
    "$$V(D) = \\mathbb{E}_{x \\sim p_{data}} [\\log D(x)] + \\mathbb{E}_{x \\sim p_G} [\\log(1 - D(x))]$$\n",
    "\n",
    "When $p_G = p_{data}$, we can write:\n",
    "$$V(D) = \\mathbb{E}_{x \\sim p_{data}} [\\log D(x) + \\log(1 - D(x))]$$\n",
    "\n",
    "Taking the derivative with respect to $D(x)$ and setting it to zero:\n",
    "$$\\frac{\\partial}{\\partial D(x)} [\\log D(x) + \\log(1 - D(x))] = \\frac{1}{D(x)} - \\frac{1}{1-D(x)} = 0$$\n",
    "\n",
    "Solving: $D(x) = \\frac{1}{2}$\n",
    "\n",
    "**Is this an optimum?**\n",
    "\n",
    "Yes, this is a Nash equilibrium:\n",
    "- The discriminator cannot distinguish between real and fake samples (both have 50% probability)\n",
    "- The generator cannot improve further since it already matches the true distribution perfectly\n",
    "- This represents the theoretical optimal solution to the GAN game\n",
    "\n",
    "However, in practice, achieving this equilibrium is difficult due to:\n",
    "- Non-convex optimization landscape\n",
    "- Instability in simultaneous gradient descent\n",
    "- Finite sample effects and model capacity limitations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "latent_dim = 100\n",
    "label_dim = len(classes)\n",
    "\n",
    "# The generator takes random `latent` noise and\n",
    "# a label it into an MNIST image conditioned on label\n",
    "# p(x|y).\n",
    "class ConditionalGenerator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConditionalGenerator, self).__init__()\n",
    "        \n",
    "        self.conv_z = nn.Sequential(\n",
    "            nn.ConvTranspose2d(latent_dim, 256, kernel_size=3, stride=2),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.conv_y = nn.Sequential(\n",
    "            nn.ConvTranspose2d(label_dim, 256, kernel_size=3, stride=2),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.ConvTranspose2d(512, 128, kernel_size=3, stride=2),  # 512 because we concatenate z and y\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64, 1, kernel_size=2, stride=2),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def forward(self, z, y):\n",
    "        # Process z and y separately through their respective conv layers\n",
    "        z_out = self.conv_z(z)\n",
    "        # Reshape y to have spatial dimensions for concatenation\n",
    "        # y is (batch_size, label_dim), we need (batch_size, label_dim, 1, 1)\n",
    "        y_reshaped = y.unsqueeze(-1).unsqueeze(-1)\n",
    "        y_out = self.conv_y(y_reshaped)\n",
    "        \n",
    "        # Merge information and send through network\n",
    "        x = torch.cat([z_out, y_out], dim=1)\n",
    "        x = self.model(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class ConditionalDiscriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConditionalDiscriminator, self).__init__()\n",
    "        self.conv_x = nn.Sequential(\n",
    "            nn.Conv2d(1, 64, kernel_size=4, stride=2),\n",
    "            nn.LeakyReLU(0.2)\n",
    "        )\n",
    "        \n",
    "        self.conv_y = nn.Sequential(\n",
    "            nn.Conv2d(label_dim, 64, kernel_size=4, stride=2),\n",
    "            nn.LeakyReLU(0.2)\n",
    "        )\n",
    "        \n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(128, 128, kernel_size=4, stride=2),  # 128 because we concatenate x and y\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(128, 256, kernel_size=4, stride=2),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            Flatten(),\n",
    "            nn.Linear(256, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x, y):\n",
    "        # Process x through conv\n",
    "        x_out = self.conv_x(x)\n",
    "        \n",
    "        # Reshape y to have spatial dimensions matching x\n",
    "        # y is (batch_size, label_dim), expand to (batch_size, label_dim, 28, 28)\n",
    "        y_expanded = y.unsqueeze(-1).unsqueeze(-1).expand(-1, -1, x.size(2), x.size(3))\n",
    "        y_out = self.conv_y(y_expanded)\n",
    "        \n",
    "        # Merge information and send through network\n",
    "        combined = torch.cat([x_out, y_out], dim=1)\n",
    "        output = self.model(combined)\n",
    "        return output\n",
    "\n",
    "loss = nn.BCELoss()\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "generator = ConditionalGenerator().to(device)\n",
    "discriminator = ConditionalDiscriminator().to(device)\n",
    "\n",
    "generator_optim = torch.optim.Adam(generator.parameters(), 2e-4, betas=(0.5, 0.999))\n",
    "discriminator_optim = torch.optim.Adam(discriminator.parameters(), 2e-4, betas=(0.5, 0.999))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional Exercise 2: Conditional GANs\n",
    "Take a look at conditional GANs [[Mirza and Osindero, 2014]](https://arxiv.org/abs/1411.1784). In essence, we add additional information through the variable $y$ to the GAN\n",
    "\n",
    "* Assume that the following about the mutual information between $X$ and $Y$: $I(X, Y) > 0$. Now prove that knowing $Y$ reduces our uncertainty about $X$, equivalently $H(X|Y) \\leq H(X)$. Explain why this makes the GAN better.\n",
    "* Explain how a conditional GAN can be used for semi-supervised learning. How would you formulate the objective (loss) function?\n",
    "* Implement a conditional GAN by feeding in the label information for each digit into the generator and discriminator. You can use the code below as a starting point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution\n",
    "\n",
    "### Question 1: Mutual Information and Conditional Entropy\n",
    "\n",
    "**Goal**: Prove that $H(X|Y) \\leq H(X)$ when $I(X, Y) > 0$.\n",
    "\n",
    "**Proof**:\n",
    "\n",
    "Recall that mutual information is defined as:\n",
    "$$I(X, Y) = H(X) - H(X|Y) = H(Y) - H(Y|X)$$\n",
    "\n",
    "Rearranging:\n",
    "$$H(X|Y) = H(X) - I(X, Y)$$\n",
    "\n",
    "Since we're given that $I(X, Y) > 0$, we have:\n",
    "$$H(X|Y) = H(X) - I(X, Y) < H(X)$$\n",
    "\n",
    "Therefore, $H(X|Y) \\leq H(X)$ (with equality only when $I(X, Y) = 0$, i.e., when $X$ and $Y$ are independent).\n",
    "\n",
    "**Why this makes the GAN better**:\n",
    "\n",
    "1. **Reduced Uncertainty**: Conditional entropy $H(X|Y)$ represents the uncertainty about $X$ given $Y$. When $H(X|Y) < H(X)$, knowing the label $Y$ reduces our uncertainty about the image $X$.\n",
    "\n",
    "2. **Easier Learning Task**: For the generator:\n",
    "   - Instead of learning $p(X)$ (all possible images), it learns $p(X|Y)$ (images conditioned on labels)\n",
    "   - Each conditional distribution $p(X|Y=y)$ is simpler than the joint distribution\n",
    "   - The generator can focus on generating specific types of images for each label\n",
    "\n",
    "3. **Better Mode Coverage**: Regular GANs often suffer from mode collapse. Conditional GANs are less prone to this because:\n",
    "   - The model must generate diverse samples for each class\n",
    "   - The conditioning forces the generator to explore different modes of the distribution\n",
    "\n",
    "4. **Discriminator Task**: The discriminator also benefits from the additional information, leading to more stable training.\n",
    "\n",
    "### Question 2: Conditional GANs for Semi-Supervised Learning\n",
    "\n",
    "**How to use conditional GANs for semi-supervised learning**:\n",
    "\n",
    "In semi-supervised learning, we have:\n",
    "- A small labeled dataset: $\\{(x_i, y_i)\\}_{i=1}^{N_L}$\n",
    "- A larger unlabeled dataset: $\\{x_j\\}_{j=1}^{N_U}$ where $N_U \\gg N_L$\n",
    "\n",
    "**Approach**:\n",
    "\n",
    "1. **Modify the discriminator** to perform multi-class classification (K+1 classes):\n",
    "   - K classes for real data with labels\n",
    "   - 1 class for fake/generated data\n",
    "   - Output: $D(x) \\in \\mathbb{R}^{K+1}$ (logits for each class including \"fake\")\n",
    "\n",
    "2. **Three loss components**:\n",
    "\n",
    "   a) **Supervised loss** (on labeled real data):\n",
    "   $$\\mathcal{L}_{supervised} = -\\mathbb{E}_{(x,y) \\sim p_{labeled}} [\\log p_{model}(y|x, y \\leq K)]$$\n",
    "   \n",
    "   b) **Unsupervised loss on real data** (discriminator should not classify as fake):\n",
    "   $$\\mathcal{L}_{unsup\\_real} = -\\mathbb{E}_{x \\sim p_{data}} [\\log(1 - p_{model}(y=K+1|x))]$$\n",
    "   \n",
    "   c) **Unsupervised loss on fake data** (discriminator should classify as fake):\n",
    "   $$\\mathcal{L}_{unsup\\_fake} = -\\mathbb{E}_{z \\sim p(z), y \\sim p(y)} [\\log p_{model}(y=K+1|G(z, y))]$$\n",
    "\n",
    "3. **Generator loss**:\n",
    "   $$\\mathcal{L}_G = -\\mathbb{E}_{z \\sim p(z), y \\sim p(y)} [\\log(1 - p_{model}(y=K+1|G(z, y)))]$$\n",
    "   \n",
    "   Or equivalently, encourage the generator to produce samples that the discriminator classifies as real with the correct label:\n",
    "   $$\\mathcal{L}_G = -\\mathbb{E}_{z \\sim p(z), y \\sim p(y)} [\\log p_{model}(y|G(z, y), y \\leq K)]$$\n",
    "\n",
    "**Total discriminator objective**:\n",
    "$$\\mathcal{L}_D = \\mathcal{L}_{supervised} + \\mathcal{L}_{unsup\\_real} + \\mathcal{L}_{unsup\\_fake}$$\n",
    "\n",
    "**Benefits**:\n",
    "- The discriminator learns good features for classification using both labeled and unlabeled data\n",
    "- The generator creates realistic samples that help regularize the decision boundary\n",
    "- Unlabeled data helps learn better representations through the adversarial game\n",
    "\n",
    "### Question 3: Implementation\n",
    "\n",
    "See the code cells below for the complete implementation of conditional GAN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "tmp_img = \"tmp_cgan_out.png\"\n",
    "discriminator_loss, generator_loss = [], []\n",
    "\n",
    "num_epochs = 50\n",
    "for epoch in range(num_epochs):\n",
    "    batch_d_loss, batch_g_loss = [], []\n",
    "    \n",
    "    for x, y in train_loader:\n",
    "        batch_size = x.size(0)\n",
    "        # True data is given label 1, while fake data is given label 0\n",
    "        true_label = torch.ones(batch_size, 1).to(device)\n",
    "        fake_label = torch.zeros(batch_size, 1).to(device)\n",
    "\n",
    "        x_true = Variable(x).to(device)\n",
    "        y = Variable(y).to(device)\n",
    "        \n",
    "        # Create random noise\n",
    "        z = torch.randn(batch_size, latent_dim, 1, 1)\n",
    "        z = Variable(z, requires_grad=False).to(device)\n",
    "        \n",
    "        discriminator.zero_grad()\n",
    "        generator.zero_grad()\n",
    "        \n",
    "        # Step 1. Send real data (with labels) through discriminator\n",
    "        #         and backpropagate its errors.\n",
    "        output = discriminator(x_true, y)\n",
    "        error_true = loss(output, true_label)\n",
    "        error_true.backward()\n",
    "        \n",
    "        # Step 2. Generate fake data G(z, y), conditioned on label y\n",
    "        x_fake = generator(z, y)\n",
    "        \n",
    "        # Step 3. Send fake data (with labels) through discriminator\n",
    "        #         propagate error and update D weights.\n",
    "        output = discriminator(x_fake.detach(), y)\n",
    "        error_fake = loss(output, fake_label)\n",
    "        error_fake.backward()\n",
    "        discriminator_optim.step()\n",
    "        \n",
    "        # Step 4. Send fake data through discriminator _again_\n",
    "        #         propagate the error of the generator and update G weights.\n",
    "        output = discriminator(x_fake, y)\n",
    "        error_generator = loss(output, true_label)\n",
    "        error_generator.backward()\n",
    "        generator_optim.step()\n",
    "        \n",
    "        batch_d_loss.append((error_true/(error_true + error_fake)).item())\n",
    "        batch_g_loss.append(error_generator.item())\n",
    "\n",
    "    discriminator_loss.append(np.mean(batch_d_loss))\n",
    "    generator_loss.append(np.mean(batch_g_loss))\n",
    "    \n",
    "    # -- Plotting --\n",
    "    f, axarr = plt.subplots(1, 2, figsize=(18, 7))\n",
    "\n",
    "    # Loss\n",
    "    ax = axarr[0]\n",
    "    ax.set_xlabel('Epoch')\n",
    "    ax.set_ylabel('Loss')\n",
    "\n",
    "    ax.plot(np.arange(epoch+1), discriminator_loss)\n",
    "    ax.plot(np.arange(epoch+1), generator_loss, linestyle=\"--\")\n",
    "    ax.legend(['Discriminator', 'Generator'])\n",
    "    \n",
    "    # Latent space samples - generate samples for each class\n",
    "    ax = axarr[1]\n",
    "    ax.set_title('Conditional samples from generator (by class)')\n",
    "    ax.axis('off')\n",
    "\n",
    "    rows, columns = 8, 8\n",
    "    \n",
    "    # Generate data conditioned on different labels\n",
    "    with torch.no_grad():\n",
    "        z = torch.randn(rows*columns, latent_dim, 1, 1)\n",
    "        z = Variable(z, requires_grad=False).to(device)\n",
    "        \n",
    "        # Create labels - alternate between classes\n",
    "        labels_list = []\n",
    "        for i in range(rows*columns):\n",
    "            label_idx = i % len(classes)\n",
    "            label_onehot = torch.zeros(label_dim)\n",
    "            label_onehot[label_idx] = 1\n",
    "            labels_list.append(label_onehot)\n",
    "        y_gen = torch.stack(labels_list).to(device)\n",
    "        \n",
    "        x_fake = generator(z, y_gen)\n",
    "    \n",
    "    canvas = np.zeros((28*rows, columns*28))\n",
    "    for i in range(rows):\n",
    "        for j in range(columns):\n",
    "            idx = i % columns + rows * j\n",
    "            canvas[i*28:(i+1)*28, j*28:(j+1)*28] = x_fake.data[idx].cpu()\n",
    "    ax.imshow(canvas, cmap='gray')\n",
    "    \n",
    "    plt.savefig(tmp_img)\n",
    "    plt.close(f)\n",
    "    display(Image(filename=tmp_img))\n",
    "    clear_output(wait=True)\n",
    "\n",
    "    os.remove(tmp_img)\n",
    "    \n",
    "print(\"Training complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
