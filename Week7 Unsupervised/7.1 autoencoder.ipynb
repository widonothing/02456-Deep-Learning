{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Image, display, clear_output\n",
    "import numpy as np\n",
    "%matplotlib nbagg\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_palette(sns.dark_palette(\"purple\"))\n",
    "\n",
    "try:\n",
    "    from plotting import plot_autoencoder_stats\n",
    "except Exception as ex:\n",
    "    print(f\"If using Colab, you may need to upload `plotting.py`. \\\n",
    "          \\nIn the left pannel, click `Files > upload to session storage` and select the file `plotting.py` from your computer \\\n",
    "          \\n---------------------------------------------\")\n",
    "    print(ex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupservised Learning \n",
    "\n",
    "## Labelling Data is Expensive\n",
    "\n",
    "In supervised machine learning, one aims at learning a mapping $f_{\\psi} : \\mathbf{x} \\in \\mathcal{R}^P \\rightarrow \\mathbf{y}$ from observations $\\mathbf{x}$ to the target $\\mathbf{y}$ using a dataset $\\mathcal{D} = \\{\\mathbf{x}_i, \\mathbf{y}_i\\}_{i=1, \\dots, N}$ of finite size N (e.g. image classification, translation). Because of the curse of dimensionality, high-dimensional inputs (images) and complex the models (deep learning) require large datasets (million of pairs $(\\mathbf{x}, \\mathbf{y})$). In practice, labelling data is expensive (e.g. marking the presence of cancer in X-ray chest scans). \n",
    "\n",
    "## Compression is Comprehension: Learning without Target\n",
    "\n",
    "In order to overcome the curse of dimensionality, we aim at learning a compressed representation $\\mathbf{z} \\in \\mathcal{R}^M$ of $\\mathbf{x}$ such that $M < P$ and there is a mapping $g_{\\phi}: \\mathbf{x} \\rightarrow \\mathbf{z}$ linking each data point to its representation. Ideally, $\\mathbf{z}$ is low-dimensional set of features which efficiently describes $\\mathbf{x}$. As an illustration, when modelling pictures of celebrities (CelebA dataset), the set of facial features (eye color, age, hair lenth, etc.) is a compressed (and lossy) representation of $\\mathbf{x}$. In practice, the representation  $\\mathbf{z}$ is unobservable and [unlikely to overlap with such known features](https://arxiv.org/abs/1811.12359). Yet, the representation $\\mathbf{z}$ is low dimensional and learning a mapping $f_{\\psi} : \\mathbf{z} \\in \\mathcal{R}^M \\rightarrow \\mathbf{y}$ is often easier.\n",
    "\n",
    "Whereas labelling the data is expensive, observations $\\mathbf{x}$ are cheap to acquire. In many cases, one can scrap the web to gather a large collection of images or text. As a result, large deep learning models can be deployed to learn $g_{\\phi}$, and smaller / data-efficient models can be applied downstream to solve the supervised task.\n",
    "\n",
    "\n",
    "\n",
    "# Auto-encoders: Compression as a Generation Process\n",
    "In this notebook you will implement a simple auto-encoder (AE). We assume that you are already familiar with the basics of neural networks. We will start by defining an AE similar to the one used for the finetuning step by [Geoffrey Hinton and Ruslan Salakhutdinov](https://www.cs.toronto.edu/~hinton/science.pdf). We will experiment with the AE setup and try to run it on the MNIST dataset. There has been a wide variety of research into the field of auto-encoders and the technique that you are about to learn is very simple compared to modern methods: Masked Autoencoders ([MADE](https://arxiv.org/abs/1502.03509), [BERT](https://arxiv.org/abs/1810.04805)) and Variational Autoencoders ([VAE](https://arxiv.org/abs/1312.6114), [VQ-VAE](https://arxiv.org/abs/1711.00937), [BIVA](https://arxiv.org/abs/1902.02102), [NVAE](https://arxiv.org/abs/2007.03898)).\n",
    "\n",
    "In unsupervised learning, we aim at learning compressed representations $\\mathbf{z} \\in \\mathcal{P}$ of $\\mathbf{x} \\in \\mathcal{R}$ where $ M < P$. The architecture of an autoencoder can be decomposed in two steps:\n",
    "\n",
    "1. *Encoding* $\\mathbf{x}$ into a low-dimensional representation $\\mathbf{z}$ using a neural network $g_{\\phi} : \\mathbf{x} \\rightarrow \\mathbf{z}$.\n",
    "2. *Decoding* the representation $\\mathbf{z}$ into a reconstruction $\\hat{\\mathbf{x}} = h_\\theta(\\mathbf{z}) \\in \\mathcal{R}^P$.\n",
    "\n",
    "Because $M < P$, the encoding acts as an information bottleneck: only part of the information describing $\\mathbf{x}$ can be encoded into $\\mathbf{z}$ as long as $M$ is sufficiently small.\n",
    "\n",
    "Learning the parameters of the autoencoder relies on two aspects:\n",
    "\n",
    "1. A distance in the observation space $d : \\mathcal{R}^{P} \\times \\mathcal{R}^{P} \\rightarrow \\mathcal{R}$ (e.g. MSE), measuring the reconstruction quality.\n",
    "2. Using backpropagation coupled with stochastic gradient descent (SGD) to optimize the parameters $\\{\\phi, \\theta\\}$ w.r.t $L := \\frac{1}{N} \\sum_i d(x_i, h_{\\theta}(g_{\\phi}(\\mathbf{x})))$.\n",
    "\n",
    "<img src=\"static/autoencoder.png\" />\n",
    "\n",
    "*The exercises are found at the bottom of the notebook*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST\n",
    "First let us load the MNIST dataset and plot a few examples. In this notebook we will use the *dataloaders* and *datasets* provided by PyTorch. Defining the loading of datasets using a dataloader has the advantage that it only load the data that is *neccessary* into memory, which enables us to use very large scale datasets.\n",
    "\n",
    "We only load a limited amount of classes defined by the `classes` variable to speed up training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "cuda = torch.cuda.is_available()\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "# Flatten the 2d-array image into a vector\n",
    "flatten = lambda x: ToTensor()(x).view(28**2)\n",
    "\n",
    "# Define the train and test sets\n",
    "dset_train = MNIST(\"./\", train=True,  transform=flatten, download=True)\n",
    "dset_test  = MNIST(\"./\", train=False, transform=flatten)\n",
    "\n",
    "# The digit classes to use\n",
    "classes = [3, 7]\n",
    "\n",
    "def stratified_sampler(labels, classes):\n",
    "    \"\"\"Sampler that only picks datapoints corresponding to the specified classes\"\"\"\n",
    "    from functools import reduce\n",
    "    (indices,) = np.where(reduce(lambda x, y: x | y, [labels.numpy() == i for i in classes]))\n",
    "    indices = torch.from_numpy(indices)\n",
    "    return SubsetRandomSampler(indices)\n",
    "\n",
    "\n",
    "# The loaders perform the actual work\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(dset_train, batch_size=batch_size,\n",
    "                          sampler=stratified_sampler(dset_train.targets, classes), pin_memory=cuda)\n",
    "test_loader  = DataLoader(dset_test, batch_size=batch_size, \n",
    "                          sampler=stratified_sampler(dset_test.targets, classes), pin_memory=cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a batch of MNIST examples\n",
    "f, axarr = plt.subplots(4, 16, figsize=(16, 4))\n",
    "\n",
    "# Load a batch of images into memory\n",
    "images, labels = next(iter(train_loader))\n",
    "\n",
    "for i, ax in enumerate(axarr.flat):\n",
    "    ax.imshow(images[i].view(28, 28), cmap=\"binary_r\")\n",
    "    ax.axis('off')\n",
    "    \n",
    "plt.suptitle('MNIST handwritten digits')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the model\n",
    "When defining the model the latent layer $z$ must act as a bottleneck of information. We initialize the AE with 1 hidden layer in the encoder and decoder using ReLU units as nonlinearities. The latent layer has a dimensionality of 2 in order to make it easy to visualise. Since $x$ are pixel intensities that are normalized between 0 and 1, we use the sigmoid nonlinearity to model the reconstruction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "# define size variables\n",
    "num_features = 28*28\n",
    "\n",
    "class AutoEncoder(nn.Module):\n",
    "    def __init__(self, hidden_units, latent_features=2):\n",
    "        super(AutoEncoder, self).__init__()\n",
    "        # We typically employ an \"hourglass\" structure\n",
    "        # meaning that the decoder should be an encoder\n",
    "        # in reverse.\n",
    "        \n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(in_features=num_features, out_features=hidden_units),\n",
    "            nn.ReLU(),\n",
    "            # bottleneck layer\n",
    "            nn.Linear(in_features=hidden_units, out_features=latent_features)\n",
    "        )\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(in_features=latent_features, out_features=hidden_units),\n",
    "            nn.ReLU(),\n",
    "            # output layer, projecting back to image size\n",
    "            nn.Linear(in_features=hidden_units, out_features=num_features)\n",
    "        )\n",
    "\n",
    "    def forward(self, x): \n",
    "        outputs = {}\n",
    "        # we don't apply an activation to the bottleneck layer\n",
    "        z = self.encoder(x)\n",
    "        \n",
    "        # apply sigmoid to output to get pixel intensities between 0 and 1\n",
    "        x_hat = torch.sigmoid(self.decoder(z))\n",
    "        \n",
    "        return {\n",
    "            'z': z,\n",
    "            'x_hat': x_hat\n",
    "        }\n",
    "\n",
    "\n",
    "# Choose the shape of the autoencoder\n",
    "net = AutoEncoder(hidden_units=128, latent_features=2)\n",
    "\n",
    "if cuda:\n",
    "    net = net.cuda()\n",
    "\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following we define the PyTorch functions for training and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# if you want L2 regularization, then add weight_decay to SGD\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.25)\n",
    "\n",
    "# We will use pixel wise mean-squared error as our loss function\n",
    "loss_function = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can test the forward pass by checking whether the output shape is the same as the as the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the forward pass\n",
    "# expect output size of [32, num_features]\n",
    "x, y = next(iter(train_loader))\n",
    "print(f\"x.shape = {x.shape}\")\n",
    "\n",
    "if cuda:\n",
    "    x = x.cuda()\n",
    "\n",
    "outputs = net(x)\n",
    "print(f\"x_hat.shape = {outputs['x_hat'].shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the training loop we sample each batch and evaluate the error, latent space, and reconstructions on every epoch.\n",
    "\n",
    "**NOTE** this will take a while on CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "num_epochs = 100\n",
    "\n",
    "train_loss = []\n",
    "valid_loss = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    batch_loss = []\n",
    "    net.train()\n",
    "    \n",
    "    # Go through each batch in the training dataset using the loader\n",
    "    # Note that y is not necessarily known as it is here\n",
    "    for x, y in train_loader:\n",
    "        \n",
    "        if cuda:\n",
    "            x = x.cuda()\n",
    "        \n",
    "        outputs = net(x)\n",
    "        x_hat = outputs['x_hat']\n",
    "\n",
    "        # note, target is the original tensor, as we're working with auto-encoders\n",
    "        loss = loss_function(x_hat, x)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        batch_loss.append(loss.item())\n",
    "\n",
    "    train_loss.append(np.mean(batch_loss))\n",
    "\n",
    "    # Evaluate, do not propagate gradients\n",
    "    with torch.no_grad():\n",
    "        net.eval()\n",
    "        \n",
    "        # Just load a single batch from the test loader\n",
    "        x, y = next(iter(test_loader))\n",
    "        \n",
    "        if cuda:\n",
    "            x = x.cuda()\n",
    "        \n",
    "        outputs = net(x)\n",
    "\n",
    "        # We save the latent variable and reconstruction for later use\n",
    "        # we will need them on the CPU to plot\n",
    "        x_hat = outputs['x_hat']\n",
    "        z = outputs['z'].cpu().numpy()\n",
    "\n",
    "        loss = loss_function(x_hat, x)\n",
    "\n",
    "        valid_loss.append(loss.item())\n",
    "    \n",
    "    if epoch == 0:\n",
    "        continue\n",
    "\n",
    "    # live plotting of the trainig curves and representation\n",
    "    plot_autoencoder_stats(x=x.cpu(),\n",
    "                           x_hat=x_hat.cpu(),\n",
    "                           z=z,\n",
    "                           y=y,\n",
    "                           train_loss=train_loss,\n",
    "                           valid_loss=valid_loss,\n",
    "                           epoch=epoch,\n",
    "                           classes=classes,\n",
    "                           dimensionality_reduction_op=None)\n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Analyzing the AE\n",
    "1. The above implementation of an AE is very simple.\n",
    "    - Experiment with the number of layers and try different non-linearities in order to improve the reconstructions\n",
    "    - What happens with the network when we change the non-linearities in the latent layer (e.g. sigmoid)?\n",
    "    - Try to increase the number of digit classes in the training set and analyze the results\n",
    "    - Test different optimization algorithms such as ADAM and RMSProp and decide whether you should use regularizers\n",
    "       \n",
    "2. Currently we optimize w.r.t. mean squared error. \n",
    "    - Find another error function that could fit this problem better\n",
    "    - Evaluate whether the similarity function $d$ is a better choice and explain your findings\n",
    "\n",
    "3. Complexity of the bottleneck.\n",
    "    - Increase the number of units in the latent layer and train\n",
    "    - Visualize by using [PCA](http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html) or [t-SNE](http://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solutions to Exercise 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1: Deeper AutoEncoder with Different Non-linearities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeeperAutoEncoder(nn.Module):\n",
    "    \"\"\"AutoEncoder with more layers and different activation options\"\"\"\n",
    "    def __init__(self, hidden_units=[512, 256, 128], latent_features=2, activation='relu'):\n",
    "        super(DeeperAutoEncoder, self).__init__()\n",
    "        \n",
    "        # Select activation function\n",
    "        activations = {\n",
    "            'relu': nn.ReLU(),\n",
    "            'leaky_relu': nn.LeakyReLU(0.2),\n",
    "            'elu': nn.ELU(),\n",
    "            'tanh': nn.Tanh()\n",
    "        }\n",
    "        self.act = activations.get(activation, nn.ReLU())\n",
    "        \n",
    "        # Build encoder layers\n",
    "        encoder_layers = []\n",
    "        in_features = num_features\n",
    "        for hidden in hidden_units:\n",
    "            encoder_layers.extend([\n",
    "                nn.Linear(in_features, hidden),\n",
    "                activations.get(activation, nn.ReLU())\n",
    "            ])\n",
    "            in_features = hidden\n",
    "        # Bottleneck layer (no activation)\n",
    "        encoder_layers.append(nn.Linear(in_features, latent_features))\n",
    "        self.encoder = nn.Sequential(*encoder_layers)\n",
    "        \n",
    "        # Build decoder layers (reverse of encoder)\n",
    "        decoder_layers = []\n",
    "        in_features = latent_features\n",
    "        for hidden in reversed(hidden_units):\n",
    "            decoder_layers.extend([\n",
    "                nn.Linear(in_features, hidden),\n",
    "                activations.get(activation, nn.ReLU())\n",
    "            ])\n",
    "            in_features = hidden\n",
    "        # Output layer\n",
    "        decoder_layers.append(nn.Linear(in_features, num_features))\n",
    "        self.decoder = nn.Sequential(*decoder_layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        z = self.encoder(x)\n",
    "        x_hat = torch.sigmoid(self.decoder(z))\n",
    "        return {'z': z, 'x_hat': x_hat}\n",
    "\n",
    "# Test with LeakyReLU\n",
    "net_deeper = DeeperAutoEncoder(hidden_units=[512, 256, 128], latent_features=2, activation='leaky_relu')\n",
    "if cuda:\n",
    "    net_deeper = net_deeper.cuda()\n",
    "print(net_deeper)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analysis:** \n",
    "- **More layers**: Deeper networks can learn more complex representations but may be harder to train\n",
    "- **LeakyReLU**: Prevents dying ReLU problem by allowing small negative gradients\n",
    "- **ELU**: Smoother activation, can speed up training\n",
    "- **Tanh**: Outputs in [-1, 1], can be better for centered data but may cause saturation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2: Effect of Non-linearities in the Latent Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AutoEncoder with non-linearity in latent layer\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "class AutoEncoderWithLatentActivation(nn.Module):\n",
    "    def __init__(self, input_size=784, hidden_size=128, latent_size=2, latent_activation='tanh'):\n",
    "        super(AutoEncoderWithLatentActivation, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, latent_size),\n",
    "        )\n",
    "        \n",
    "        # Add activation in latent space\n",
    "        if latent_activation == 'tanh':\n",
    "            self.latent_activation = nn.Tanh()\n",
    "        elif latent_activation == 'sigmoid':\n",
    "            self.latent_activation = nn.Sigmoid()\n",
    "        elif latent_activation == 'relu':\n",
    "            self.latent_activation = nn.ReLU()\n",
    "        else:\n",
    "            self.latent_activation = nn.Identity()\n",
    "        \n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, input_size),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        z = self.encoder(x)\n",
    "        z = self.latent_activation(z)\n",
    "        x_reconstructed = self.decoder(z)\n",
    "        return x_reconstructed, z\n",
    "\n",
    "# Test different latent activations\n",
    "latent_activations = ['none', 'tanh', 'sigmoid', 'relu']\n",
    "results_latent = {}\n",
    "\n",
    "for activation in latent_activations:\n",
    "    print(f\"\\nTraining with latent activation: {activation}\")\n",
    "    model = AutoEncoderWithLatentActivation(latent_activation=activation)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    \n",
    "    train_losses = []\n",
    "    for epoch in range(20):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for x, _ in train_loader:\n",
    "            x = x.view(-1, 784)\n",
    "            optimizer.zero_grad()\n",
    "            x_recon, _ = model(x)\n",
    "            loss = F.mse_loss(x_recon, x)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        train_losses.append(avg_loss)\n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            print(f\"Epoch {epoch+1}, Loss: {avg_loss:.6f}\")\n",
    "    \n",
    "    results_latent[activation] = {\n",
    "        'model': model,\n",
    "        'train_losses': train_losses\n",
    "    }\n",
    "\n",
    "# Plot comparison\n",
    "plt.figure(figsize=(12, 4))\n",
    "for activation, result in results_latent.items():\n",
    "    plt.plot(result['train_losses'], label=f'{activation}')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('MSE Loss')\n",
    "plt.title('Effect of Latent Layer Activation Functions')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nFinal losses:\")\n",
    "for activation, result in results_latent.items():\n",
    "    print(f\"{activation}: {result['train_losses'][-1]:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3: Increasing Number of Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with all 10 classes\n",
    "print(\"Training AutoEncoder with all 10 MNIST classes...\")\n",
    "\n",
    "# Load full MNIST dataset\n",
    "train_dataset_all = datasets.MNIST(root='./data', train=True, transform=transforms.ToTensor(), download=True)\n",
    "train_loader_all = torch.utils.data.DataLoader(train_dataset_all, batch_size=64, shuffle=True)\n",
    "\n",
    "# Train model\n",
    "model_all = AutoEncoder()\n",
    "optimizer = optim.Adam(model_all.parameters(), lr=0.001)\n",
    "\n",
    "train_losses_all = []\n",
    "for epoch in range(20):\n",
    "    model_all.train()\n",
    "    total_loss = 0\n",
    "    for x, _ in train_loader_all:\n",
    "        x = x.view(-1, 784)\n",
    "        optimizer.zero_grad()\n",
    "        x_recon, _ = model_all(x)\n",
    "        loss = F.mse_loss(x_recon, x)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    avg_loss = total_loss / len(train_loader_all)\n",
    "    train_losses_all.append(avg_loss)\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        print(f\"Epoch {epoch+1}, Loss: {avg_loss:.6f}\")\n",
    "\n",
    "# Visualize latent space with all classes\n",
    "model_all.eval()\n",
    "latent_vectors_all = []\n",
    "labels_all = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for x, y in train_loader_all:\n",
    "        x = x.view(-1, 784)\n",
    "        _, z = model_all(x)\n",
    "        latent_vectors_all.append(z)\n",
    "        labels_all.append(y)\n",
    "        if len(latent_vectors_all) * 64 >= 5000:  # Sample 5000 points\n",
    "            break\n",
    "\n",
    "latent_vectors_all = torch.cat(latent_vectors_all).numpy()\n",
    "labels_all = torch.cat(labels_all).numpy()\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "scatter = plt.scatter(latent_vectors_all[:, 0], latent_vectors_all[:, 1], \n",
    "                     c=labels_all, cmap='tab10', alpha=0.5, s=5)\n",
    "plt.colorbar(scatter, ticks=range(10))\n",
    "plt.xlabel('Latent Dimension 1')\n",
    "plt.ylabel('Latent Dimension 2')\n",
    "plt.title('Latent Space Visualization - All 10 MNIST Classes')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nFinal loss with 10 classes: {train_losses_all[-1]:.6f}\")\n",
    "print(f\"Compare to 2 classes: {train_losses[-1]:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4: Different Optimizers and Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different optimizers\n",
    "optimizers_configs = {\n",
    "    'Adam': lambda params: optim.Adam(params, lr=0.001),\n",
    "    'SGD': lambda params: optim.SGD(params, lr=0.01, momentum=0.9),\n",
    "    'RMSprop': lambda params: optim.RMSprop(params, lr=0.001),\n",
    "    'Adam+L2': lambda params: optim.Adam(params, lr=0.001, weight_decay=1e-5)\n",
    "}\n",
    "\n",
    "results_optimizers = {}\n",
    "\n",
    "for opt_name, opt_fn in optimizers_configs.items():\n",
    "    print(f\"\\nTraining with {opt_name}...\")\n",
    "    model = AutoEncoder()\n",
    "    optimizer = opt_fn(model.parameters())\n",
    "    \n",
    "    train_losses = []\n",
    "    for epoch in range(20):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for x, _ in train_loader:\n",
    "            x = x.view(-1, 784)\n",
    "            optimizer.zero_grad()\n",
    "            x_recon, _ = model(x)\n",
    "            loss = F.mse_loss(x_recon, x)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        train_losses.append(avg_loss)\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f\"Epoch {epoch+1}, Loss: {avg_loss:.6f}\")\n",
    "    \n",
    "    results_optimizers[opt_name] = train_losses\n",
    "\n",
    "# Plot comparison\n",
    "plt.figure(figsize=(12, 5))\n",
    "for opt_name, losses in results_optimizers.items():\n",
    "    plt.plot(losses, label=opt_name, marker='o' if len(losses) < 30 else None)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('MSE Loss')\n",
    "plt.title('Comparison of Different Optimizers')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nFinal losses:\")\n",
    "for opt_name, losses in results_optimizers.items():\n",
    "    print(f\"{opt_name}: {losses[-1]:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.5: Alternative Loss Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different loss functions\n",
    "def binary_cross_entropy_loss(x_recon, x):\n",
    "    return F.binary_cross_entropy(x_recon, x)\n",
    "\n",
    "def l1_loss(x_recon, x):\n",
    "    return F.l1_loss(x_recon, x)\n",
    "\n",
    "def smooth_l1_loss(x_recon, x):\n",
    "    return F.smooth_l1_loss(x_recon, x)\n",
    "\n",
    "loss_functions = {\n",
    "    'MSE': F.mse_loss,\n",
    "    'BCE': binary_cross_entropy_loss,\n",
    "    'L1': l1_loss,\n",
    "    'Smooth L1': smooth_l1_loss\n",
    "}\n",
    "\n",
    "results_losses = {}\n",
    "\n",
    "for loss_name, loss_fn in loss_functions.items():\n",
    "    print(f\"\\nTraining with {loss_name} loss...\")\n",
    "    model = AutoEncoder()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    \n",
    "    train_losses = []\n",
    "    for epoch in range(20):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for x, _ in train_loader:\n",
    "            x = x.view(-1, 784)\n",
    "            optimizer.zero_grad()\n",
    "            x_recon, _ = model(x)\n",
    "            loss = loss_fn(x_recon, x)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        train_losses.append(avg_loss)\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f\"Epoch {epoch+1}, Loss: {avg_loss:.6f}\")\n",
    "    \n",
    "    results_losses[loss_name] = train_losses\n",
    "\n",
    "# Plot comparison\n",
    "plt.figure(figsize=(12, 5))\n",
    "for loss_name, losses in results_losses.items():\n",
    "    plt.plot(losses, label=loss_name, marker='o' if len(losses) < 30 else None)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Comparison of Different Loss Functions')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nFinal losses:\")\n",
    "for loss_name, losses in results_losses.items():\n",
    "    print(f\"{loss_name}: {losses[-1]:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.6: Higher-Dimensional Latent Space with Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train AutoEncoder with higher-dimensional latent space\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "class AutoEncoderHighDim(nn.Module):\n",
    "    def __init__(self, input_size=784, hidden_size=128, latent_size=10):\n",
    "        super(AutoEncoderHighDim, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, latent_size),\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, input_size),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        z = self.encoder(x)\n",
    "        x_reconstructed = self.decoder(z)\n",
    "        return x_reconstructed, z\n",
    "\n",
    "print(\"Training AutoEncoder with 10D latent space...\")\n",
    "model_10d = AutoEncoderHighDim(latent_size=10)\n",
    "optimizer = optim.Adam(model_10d.parameters(), lr=0.001)\n",
    "\n",
    "train_losses_10d = []\n",
    "for epoch in range(20):\n",
    "    model_10d.train()\n",
    "    total_loss = 0\n",
    "    for x, _ in train_loader:\n",
    "        x = x.view(-1, 784)\n",
    "        optimizer.zero_grad()\n",
    "        x_recon, _ = model_10d(x)\n",
    "        loss = F.mse_loss(x_recon, x)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    train_losses_10d.append(avg_loss)\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        print(f\"Epoch {epoch+1}, Loss: {avg_loss:.6f}\")\n",
    "\n",
    "# Extract latent representations\n",
    "model_10d.eval()\n",
    "latent_vectors_10d = []\n",
    "labels_10d = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for x, y in train_loader:\n",
    "        x = x.view(-1, 784)\n",
    "        _, z = model_10d(x)\n",
    "        latent_vectors_10d.append(z)\n",
    "        labels_10d.append(y)\n",
    "\n",
    "latent_vectors_10d = torch.cat(latent_vectors_10d).numpy()\n",
    "labels_10d = torch.cat(labels_10d).numpy()\n",
    "\n",
    "# Visualize using PCA and t-SNE\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# PCA\n",
    "pca = PCA(n_components=2)\n",
    "latent_pca = pca.fit_transform(latent_vectors_10d)\n",
    "scatter1 = axes[0].scatter(latent_pca[:, 0], latent_pca[:, 1], c=labels_10d, cmap='tab10', alpha=0.5, s=5)\n",
    "axes[0].set_xlabel('PCA Component 1')\n",
    "axes[0].set_ylabel('PCA Component 2')\n",
    "axes[0].set_title(f'PCA Visualization of 10D Latent Space\\nVariance explained: {pca.explained_variance_ratio_.sum():.2%}')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "plt.colorbar(scatter1, ax=axes[0])\n",
    "\n",
    "# t-SNE (on subset for speed)\n",
    "print(\"\\nRunning t-SNE (this may take a moment)...\")\n",
    "subset_size = 2000\n",
    "indices = np.random.choice(len(latent_vectors_10d), subset_size, replace=False)\n",
    "tsne = TSNE(n_components=2, random_state=42, perplexity=30)\n",
    "latent_tsne = tsne.fit_transform(latent_vectors_10d[indices])\n",
    "scatter2 = axes[1].scatter(latent_tsne[:, 0], latent_tsne[:, 1], c=labels_10d[indices], cmap='tab10', alpha=0.5, s=5)\n",
    "axes[1].set_xlabel('t-SNE Dimension 1')\n",
    "axes[1].set_ylabel('t-SNE Dimension 2')\n",
    "axes[1].set_title('t-SNE Visualization of 10D Latent Space')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "plt.colorbar(scatter2, ax=axes[1])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nFinal loss (10D latent): {train_losses_10d[-1]:.6f}\")\n",
    "print(f\"Compare to 2D latent: {train_losses[-1]:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises 2: Adding classification for semi-supervised learning\n",
    "\n",
    "The above training has been performed unsupervised. Now let us assume that we only have a fraction of labeled data points from each class. As we know, semi-supervised learning can be utilized by combining unsupervised and supervised learning. Now you must analyze whether a trained AE from the above exercise can aid a classifier.\n",
    "\n",
    "1. Build a simple classifier (like the ones from week1) where you:\n",
    "    - Train on the labeled dataset and evaluate the results\n",
    "2. Build a second classifier and train on the latent output $\\mathbf{z}$ of the AE.\n",
    "3. Build a third classifier and train on the reconstructions of the AE.\n",
    "4. Evaluate the classifiers against each other and implement a model that improves the classification by combining the input, latent output, and reconstruction.\n",
    "\n",
    "Below we provide some starting code for using only a subset of the labelled data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solutions to Exercise 2: Semi-Supervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1: Simple Classifier on Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple classifier on raw pixel data\n",
    "class SimpleClassifier(nn.Module):\n",
    "    def __init__(self, input_size=784, hidden_size=128, num_classes=2):\n",
    "        super(SimpleClassifier, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "# Prepare test data\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, transform=transforms.ToTensor(), download=True)\n",
    "test_idx = (test_dataset.targets == 0) | (test_dataset.targets == 1)\n",
    "test_dataset.data = test_dataset.data[test_idx]\n",
    "test_dataset.targets = test_dataset.targets[test_idx]\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Train classifier on raw data\n",
    "print(\"Training classifier on raw pixel data...\")\n",
    "classifier_raw = SimpleClassifier()\n",
    "optimizer = optim.Adam(classifier_raw.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in range(10):\n",
    "    classifier_raw.train()\n",
    "    for x, y in train_loader:\n",
    "        x = x.view(-1, 784)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = classifier_raw(x)\n",
    "        loss = F.cross_entropy(outputs, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# Evaluate\n",
    "classifier_raw.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for x, y in test_loader:\n",
    "        x = x.view(-1, 784)\n",
    "        outputs = classifier_raw(x)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += y.size(0)\n",
    "        correct += (predicted == y).sum().item()\n",
    "\n",
    "accuracy_raw = 100 * correct / total\n",
    "print(f\"Accuracy on raw data: {accuracy_raw:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2: Classifier on Latent Representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classifier on latent representations\n",
    "class LatentClassifier(nn.Module):\n",
    "    def __init__(self, latent_size=2, num_classes=2):\n",
    "        super(LatentClassifier, self).__init__()\n",
    "        self.fc = nn.Linear(latent_size, num_classes)\n",
    "    \n",
    "    def forward(self, z):\n",
    "        return self.fc(z)\n",
    "\n",
    "print(\"Training classifier on latent representations...\")\n",
    "\n",
    "# Extract latent representations using trained autoencoder\n",
    "autoencoder.eval()\n",
    "latent_train = []\n",
    "labels_train = []\n",
    "with torch.no_grad():\n",
    "    for x, y in train_loader:\n",
    "        x = x.view(-1, 784)\n",
    "        _, z = autoencoder(x)\n",
    "        latent_train.append(z)\n",
    "        labels_train.append(y)\n",
    "\n",
    "latent_train = torch.cat(latent_train)\n",
    "labels_train = torch.cat(labels_train)\n",
    "\n",
    "# Create dataset from latent representations\n",
    "latent_dataset = torch.utils.data.TensorDataset(latent_train, labels_train)\n",
    "latent_loader = torch.utils.data.DataLoader(latent_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "# Train classifier\n",
    "classifier_latent = LatentClassifier()\n",
    "optimizer = optim.Adam(classifier_latent.parameters(), lr=0.01)\n",
    "\n",
    "for epoch in range(20):\n",
    "    classifier_latent.train()\n",
    "    for z, y in latent_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = classifier_latent(z)\n",
    "        loss = F.cross_entropy(outputs, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# Evaluate\n",
    "latent_test = []\n",
    "labels_test = []\n",
    "with torch.no_grad():\n",
    "    for x, y in test_loader:\n",
    "        x = x.view(-1, 784)\n",
    "        _, z = autoencoder(x)\n",
    "        latent_test.append(z)\n",
    "        labels_test.append(y)\n",
    "\n",
    "latent_test = torch.cat(latent_test)\n",
    "labels_test = torch.cat(labels_test)\n",
    "\n",
    "classifier_latent.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = classifier_latent(latent_test)\n",
    "    _, predicted = torch.max(outputs, 1)\n",
    "    correct = (predicted == labels_test).sum().item()\n",
    "    total = labels_test.size(0)\n",
    "\n",
    "accuracy_latent = 100 * correct / total\n",
    "print(f\"Accuracy on latent representations: {accuracy_latent:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3: Classifier on Reconstructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classifier on reconstructed data\n",
    "print(\"Training classifier on reconstructions...\")\n",
    "\n",
    "# Get reconstructions\n",
    "autoencoder.eval()\n",
    "recon_train = []\n",
    "labels_train = []\n",
    "with torch.no_grad():\n",
    "    for x, y in train_loader:\n",
    "        x = x.view(-1, 784)\n",
    "        x_recon, _ = autoencoder(x)\n",
    "        recon_train.append(x_recon)\n",
    "        labels_train.append(y)\n",
    "\n",
    "recon_train = torch.cat(recon_train)\n",
    "labels_train = torch.cat(labels_train)\n",
    "\n",
    "# Create dataset\n",
    "recon_dataset = torch.utils.data.TensorDataset(recon_train, labels_train)\n",
    "recon_loader = torch.utils.data.DataLoader(recon_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "# Train classifier\n",
    "classifier_recon = SimpleClassifier()\n",
    "optimizer = optim.Adam(classifier_recon.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in range(10):\n",
    "    classifier_recon.train()\n",
    "    for x_recon, y in recon_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = classifier_recon(x_recon)\n",
    "        loss = F.cross_entropy(outputs, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# Evaluate\n",
    "recon_test = []\n",
    "labels_test = []\n",
    "with torch.no_grad():\n",
    "    for x, y in test_loader:\n",
    "        x = x.view(-1, 784)\n",
    "        x_recon, _ = autoencoder(x)\n",
    "        recon_test.append(x_recon)\n",
    "        labels_test.append(y)\n",
    "\n",
    "recon_test = torch.cat(recon_test)\n",
    "labels_test = torch.cat(labels_test)\n",
    "\n",
    "classifier_recon.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = classifier_recon(recon_test)\n",
    "    _, predicted = torch.max(outputs, 1)\n",
    "    correct = (predicted == labels_test).sum().item()\n",
    "    total = labels_test.size(0)\n",
    "\n",
    "accuracy_recon = 100 * correct / total\n",
    "print(f\"Accuracy on reconstructions: {accuracy_recon:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4: Combined Classifier Using All Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combined classifier using raw data, latent, and reconstructions\n",
    "class CombinedClassifier(nn.Module):\n",
    "    def __init__(self, input_size=784, latent_size=2, hidden_size=128, num_classes=2):\n",
    "        super(CombinedClassifier, self).__init__()\n",
    "        # Combined input: raw (784) + latent (2) + reconstruction (784) = 1570\n",
    "        combined_size = input_size + latent_size + input_size\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(combined_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(hidden_size, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x, z, x_recon):\n",
    "        combined = torch.cat([x, z, x_recon], dim=1)\n",
    "        return self.fc(combined)\n",
    "\n",
    "print(\"Training combined classifier...\")\n",
    "\n",
    "# Prepare combined training data\n",
    "autoencoder.eval()\n",
    "combined_train_data = []\n",
    "labels_train = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for x, y in train_loader:\n",
    "        x = x.view(-1, 784)\n",
    "        x_recon, z = autoencoder(x)\n",
    "        combined_train_data.append((x, z, x_recon, y))\n",
    "        labels_train.append(y)\n",
    "\n",
    "# Train classifier\n",
    "classifier_combined = CombinedClassifier()\n",
    "optimizer = optim.Adam(classifier_combined.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in range(10):\n",
    "    classifier_combined.train()\n",
    "    for x, z, x_recon, y in combined_train_data:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = classifier_combined(x, z, x_recon)\n",
    "        loss = F.cross_entropy(outputs, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# Evaluate\n",
    "classifier_combined.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for x, y in test_loader:\n",
    "        x = x.view(-1, 784)\n",
    "        x_recon, z = autoencoder(x)\n",
    "        outputs = classifier_combined(x, z, x_recon)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += y.size(0)\n",
    "        correct += (predicted == y).sum().item()\n",
    "\n",
    "accuracy_combined = 100 * correct / total\n",
    "print(f\"Accuracy with combined features: {accuracy_combined:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparison and Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare all approaches\n",
    "import pandas as pd\n",
    "\n",
    "results = {\n",
    "    'Approach': ['Raw Data', 'Latent (2D)', 'Reconstructions', 'Combined'],\n",
    "    'Accuracy (%)': [accuracy_raw, accuracy_latent, accuracy_recon, accuracy_combined],\n",
    "    'Input Dimension': [784, 2, 784, 1570]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(results)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CLASSIFICATION RESULTS COMPARISON\")\n",
    "print(\"=\"*60)\n",
    "print(df.to_string(index=False))\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Visualize comparison\n",
    "plt.figure(figsize=(10, 6))\n",
    "bars = plt.bar(df['Approach'], df['Accuracy (%)'], color=['blue', 'green', 'orange', 'red'], alpha=0.7)\n",
    "plt.ylabel('Accuracy (%)', fontsize=12)\n",
    "plt.title('Classification Accuracy Comparison', fontsize=14, fontweight='bold')\n",
    "plt.ylim([90, 100])\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2., height,\n",
    "             f'{height:.2f}%',\n",
    "             ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nKey Observations:\")\n",
    "print(\"- Latent representations achieve high accuracy despite 2D compression (784â†’2)\")\n",
    "print(\"- Combined features may offer marginal improvements but increase complexity\")\n",
    "print(\"- The autoencoder learned meaningful representations for classification\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "def uniform_stratified_sampler(labels, classes, n=None):\n",
    "    \"\"\"\n",
    "    Stratified sampler that distributes labels uniformly by\n",
    "    sampling at most n data points per class\n",
    "    \"\"\"\n",
    "    from functools import reduce\n",
    "    # Only choose digits in n_labels\n",
    "    (indices,) = np.where(reduce(lambda x, y: x | y, [labels.numpy() == i for i in classes]))\n",
    "\n",
    "    # Ensure uniform distribution of labels\n",
    "    np.random.shuffle(indices)\n",
    "    indices = np.hstack([list(filter(lambda idx: labels[idx] == i, indices))[:n] for i in classes])\n",
    "\n",
    "    indices = torch.from_numpy(indices)\n",
    "    sampler = SubsetRandomSampler(indices)\n",
    "    return sampler\n",
    "\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "# Specify how many labelled examples we want per digit class\n",
    "labels_per_class = 10\n",
    "\n",
    "# Large pool of unlabelled data\n",
    "unlabelled = DataLoader(dset_train, batch_size=batch_size, \n",
    "                        sampler=stratified_sampler(dset_train.train_labels, classes=classes), pin_memory=cuda)\n",
    "\n",
    "# Smaller pool of labelled data\n",
    "labelled = DataLoader(dset_train, batch_size=batch_size,\n",
    "                      sampler=uniform_stratified_sampler(dset_train.train_labels, classes=classes, n=labels_per_class),\n",
    "                      pin_memory=cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "from itertools import cycle\n",
    "\n",
    "# Example: Semi-supervised training combining autoencoder and classifier\n",
    "# This combines unsupervised reconstruction loss and supervised classification loss\n",
    "\n",
    "class SemiSupervisedAE(nn.Module):\n",
    "    \"\"\"Autoencoder with attached classifier for semi-supervised learning\"\"\"\n",
    "    def __init__(self, hidden_units=128, latent_features=2, num_classes=len(classes)):\n",
    "        super(SemiSupervisedAE, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(num_features, hidden_units),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_units, latent_features)\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_features, hidden_units),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_units, num_features)\n",
    "        )\n",
    "        # Classifier on latent space\n",
    "        self.classifier = nn.Linear(latent_features, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        z = self.encoder(x)\n",
    "        x_hat = torch.sigmoid(self.decoder(z))\n",
    "        logits = self.classifier(z)\n",
    "        return {'z': z, 'x_hat': x_hat, 'logits': logits}\n",
    "\n",
    "# Initialize model\n",
    "semi_model = SemiSupervisedAE(hidden_units=128, latent_features=8)\n",
    "if cuda:\n",
    "    semi_model = semi_model.cuda()\n",
    "\n",
    "optimizer_semi = optim.Adam(semi_model.parameters(), lr=0.001)\n",
    "recon_loss_fn = nn.BCELoss()\n",
    "class_loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 50\n",
    "alpha = 0.1  # Weight for classification loss\n",
    "\n",
    "print(\"Semi-supervised training: combining reconstruction and classification...\")\n",
    "for epoch in range(num_epochs):\n",
    "    semi_model.train()\n",
    "    \n",
    "    # Go through both labelled and unlabelled data\n",
    "    for (x_l, y_l), (x_u, _) in zip(cycle(labelled), unlabelled):\n",
    "        \n",
    "        if cuda:\n",
    "            x_l, y_l, x_u = x_l.cuda(), y_l.cuda(), x_u.cuda()\n",
    "        \n",
    "        # Process labeled data\n",
    "        outputs_l = semi_model(x_l)\n",
    "        \n",
    "        # Reconstruction loss on labeled data\n",
    "        recon_loss_l = recon_loss_fn(outputs_l['x_hat'], x_l)\n",
    "        \n",
    "        # Classification loss on labeled data\n",
    "        y_idx = torch.tensor([classes.index(label.item()) for label in y_l])\n",
    "        if cuda:\n",
    "            y_idx = y_idx.cuda()\n",
    "        class_loss = class_loss_fn(outputs_l['logits'], y_idx)\n",
    "        \n",
    "        # Process unlabeled data (reconstruction only)\n",
    "        outputs_u = semi_model(x_u)\n",
    "        recon_loss_u = recon_loss_fn(outputs_u['x_hat'], x_u)\n",
    "        \n",
    "        # Combined loss\n",
    "        loss = recon_loss_l + recon_loss_u + alpha * class_loss\n",
    "        \n",
    "        optimizer_semi.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer_semi.step()\n",
    "    \n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        # Evaluate classification accuracy\n",
    "        semi_model.eval()\n",
    "        test_correct = 0\n",
    "        test_total = 0\n",
    "        with torch.no_grad():\n",
    "            for x, y in test_loader:\n",
    "                if cuda:\n",
    "                    x, y = x.cuda(), y.cuda()\n",
    "                outputs = semi_model(x)\n",
    "                y_idx = torch.tensor([classes.index(label.item()) for label in y])\n",
    "                if cuda:\n",
    "                    y_idx = y_idx.cuda()\n",
    "                _, predicted = torch.max(outputs['logits'].data, 1)\n",
    "                test_total += y_idx.size(0)\n",
    "                test_correct += (predicted == y_idx).sum().item()\n",
    "        \n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Test Acc: {100*test_correct/test_total:.2f}%')\n",
    "\n",
    "print(\"\\nSemi-supervised learning complete!\")\n",
    "print(\"This approach:\")\n",
    "print(\"- Trains autoencoder on ALL data (labeled + unlabeled)\")\n",
    "print(\"- Trains classifier only on labeled data\")\n",
    "print(\"- Shares representations between tasks\")\n",
    "print(\"- Typically outperforms supervised learning with limited labels\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
