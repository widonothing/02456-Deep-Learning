{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Purpose and goals\n",
    "In this notebook you will implement a simple neural network in PyTorch.\n",
    "\n",
    "The building blocks of PyTorch are Tensors, and Operations, with these we can form dynamic computational graphs that represent neural networks.\n",
    "In this exercise we'll start right away by defining a logistic regression model using these simple building blocks.\n",
    "We'll initially start with a simple 2D and binary (i.e. two-class) classification problem where the class decision boundary can be visualized.\n",
    "Initially we show that logistic regression can only separate classes linearly.\n",
    "Adding a nonlinear hidden layer to the algorithm permits nonlinear class separation.\n",
    "\n",
    "In this notebook you should:\n",
    "* **First** run the code as is, and see what it does.\n",
    "* **Then** modify the code, following the instructions in the bottom of the notebook.\n",
    "* **Lastly** play around a bit, and do some small experiments that you come up with.\n",
    "\n",
    "> We assume that you are already familiar with backpropagation (if not please see chapter 7 in the [Simon Prince book](https://udlbook.github.io/udlbook))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dependencies and supporting functions\n",
    "Load dependencies and supporting functions by running the code block below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.datasets\n",
    "\n",
    "# Do not worry about the code below for now, it is used for plotting later\n",
    "def plot_decision_boundary(pred_func, X, y):\n",
    "    # from https://github.com/dennybritz/nn-from-scratch/blob/master/nn-from-scratch.ipynb\n",
    "    # Set min and max values and give it some padding\n",
    "    x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n",
    "    y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n",
    "    \n",
    "    h = 0.01\n",
    "    # Generate a grid of points with distance h between them\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "    \n",
    "    yy = yy.astype('float32')\n",
    "    xx = xx.astype('float32')\n",
    "    # Predict the function value for the whole gid\n",
    "    Z = pred_func(np.c_[xx.ravel(), yy.ravel()])[:,0]\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    # Plot the contour and training examples\n",
    "    plt.figure()\n",
    "    plt.contourf(xx, yy, Z, cmap=plt.cm.RdBu)\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=-y, cmap=plt.cm.Spectral)\n",
    "\n",
    "def onehot(t, num_classes):\n",
    "    out = np.zeros((t.shape[0], num_classes))\n",
    "    for row, col in enumerate(t):\n",
    "        out[row, col] = 1\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Problem \n",
    "We'll initally demonstrate that Multi-layer Perceptrons (MLPs) can classify nonlinear problems, whereas a simple logistic regression model cannot.\n",
    "For ease of visualization and computational speed we initially experiment on the simple 2D half-moon dataset, visualized below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a dataset and plot it\n",
    "np.random.seed(0)\n",
    "num_samples = 300\n",
    "\n",
    "X, y = sklearn.datasets.make_moons(num_samples, noise=0.20)\n",
    "\n",
    "# define train, validation, and test sets\n",
    "X_tr = X[:100].astype('float32')\n",
    "X_val = X[100:200].astype('float32')\n",
    "X_te = X[200:].astype('float32')\n",
    "\n",
    "# and labels\n",
    "y_tr = y[:100].astype('int32')\n",
    "y_val = y[100:200].astype('int32')\n",
    "y_te = y[200:].astype('int32')\n",
    "\n",
    "plt.scatter(X_tr[:,0], X_tr[:,1], s=40, c=y_tr, cmap=plt.cm.Spectral)\n",
    "\n",
    "print(X.shape, y.shape)\n",
    "\n",
    "num_features = X_tr.shape[-1]\n",
    "num_output = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From Logistic Regression to \"Deep Learning\"\n",
    "The code implements logistic regression. In section [__Assignments Half Moon__](#Assignments-Half-Moon) (bottom of this notebook) you are asked to modify the code into a neural network.\n",
    "\n",
    "The standard building block for neural networks are layers, the simplest of which is called a *fully-connected layer* or *dense feed forward layer*, and it is computed as follows:\n",
    "\n",
    "$$y = g(W^{\\top} x + b)$$\n",
    "\n",
    "where $x$ is the input vector, $y$ is the output vector, $W, b$ are the weights and biases (a matrix and vector respectively) and $g$ is a non-linear function, called *activation function*.\n",
    "The *dense* part of the name comes from the fact that every element of $x$ contributes to every element of $y$.\n",
    "And the *feed forward* part of the name means that the layer processes each input independently. \n",
    "If we were to draw the layer it would be acyclical.\n",
    "Later we will see layers that break from both of these conventions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $x$ has shape `[batch_size, num_features]`,\n",
    "- $W$ has shape `[num_units, num_features]`,\n",
    "- $b$ has `[num_units]`, and\n",
    "- $y$ has then `[batch_size, num_units]`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch 101\n",
    "\n",
    "In this first exercise we will use basic PyTorch functions so that you can learn how to build it from scratch. This will help you later if you want to build your own custom operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[`Parameters`](https://pytorch.org/docs/stable/generated/torch.nn.parameter.Parameter.html#torch.nn.parameter.Parameter) have a very special property when used with [`Module`](https://pytorch.org/docs/stable/generated/torch.nn.Module.html?highlight=module#torch.nn.Module)s - when they’re assigned as `Module` attributes they are automatically added to the list of its parameters, and will appear e.g. in the `parameters()` iterator.\n",
    "\n",
    "Assigning a Tensor does not  have such effect. This is because one might want to cache some temporary state (more on this later) in the model. If there was no such class as `Parameter`, these temporaries would get registered too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self, hidden_units: int = 50, activation: str = \"relu\"):\n",
    "        super(Net, self).__init__()\n",
    "        # First (hidden) layer parameters\n",
    "        # W_1[out_features=hidden_units, in_features=num_features]\n",
    "        self.W_1 = nn.Parameter(torch.randn(hidden_units, num_features))\n",
    "        self.b_1 = nn.Parameter(torch.randn(hidden_units))\n",
    "\n",
    "        # Second (output) layer parameters\n",
    "        # W_2[out_features=num_output, in_features=hidden_units]\n",
    "        self.W_2 = nn.Parameter(torch.randn(num_output, hidden_units))\n",
    "        self.b_2 = nn.Parameter(torch.randn(num_output))\n",
    "\n",
    "        # Store activation choice\n",
    "        self.activation = activation\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Hidden affine + nonlinearity\n",
    "        x = F.linear(x, self.W_1, self.b_1)\n",
    "        if self.activation == \"relu\":\n",
    "            x = F.relu(x)\n",
    "        elif self.activation == \"tanh\":\n",
    "            x = torch.tanh(x)\n",
    "        elif self.activation == \"sigmoid\":\n",
    "            x = torch.sigmoid(x)\n",
    "        else:\n",
    "            # Default to identity if unknown string provided\n",
    "            pass\n",
    "\n",
    "        # Output affine\n",
    "        x = F.linear(x, self.W_2, self.b_2)\n",
    "        # Softmax over class dimension\n",
    "        return F.softmax(x, dim=1)\n",
    "\n",
    "net = Net(hidden_units=50, activation=\"relu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Knowing how to print your tensors is useful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list all parameters in your network\n",
    "print(\"NAMED PARAMETERS\")\n",
    "print(list(net.named_parameters()))\n",
    "print()\n",
    "# the .parameters() method simply gives the Tensors in the list\n",
    "print(\"PARAMETERS\")\n",
    "print(list(net.parameters()))\n",
    "print()\n",
    "\n",
    "# list individual parameters by name\n",
    "print('WEIGHTS / BIASES - LAYER 1 (hidden)')\n",
    "print(net.W_1)\n",
    "print(net.W_1.size())\n",
    "print(net.b_1)\n",
    "print(net.b_1.size())\n",
    "\n",
    "print('\\nWEIGHTS / BIASES - LAYER 2 (output)')\n",
    "print(net.W_2)\n",
    "print(net.W_2.size())\n",
    "print(net.b_2)\n",
    "print(net.b_2.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring Parameter\n",
    "\n",
    "Ok, let's investigate what a Parameter is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = net.W_1\n",
    "print(\"## this is the tensor\")\n",
    "print(param.data)\n",
    "print(\"\\n## this is the tensor's gradient\")\n",
    "print(param.grad)\n",
    "# notice, the gradient is undefined because we have not yet run a backward pass\n",
    "\n",
    "print(\"\\n## is it a leaf in the graph?\")\n",
    "print(param.is_leaf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Excluding subgraphs from backward propagation\n",
    "\n",
    "To exclude part of your computational graph (i.e. a subgraph) from backward propagation, simply set the relevant tensors' attribute `requires_grad` to `False`.\n",
    "\n",
    "If there’s a single input to an operation that requires gradient, its output will also require gradient. Conversely, only if all inputs don’t require gradient, the output also won’t require it. Backward computation is never performed in the subgraphs, where all Tensors didn’t require gradients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test network\n",
    "\n",
    "To use our network we can simply call our graph, and it will dynamically be created. Here is an example of running the network's forward pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.randn(5, num_features)\n",
    "# the net.__call__ runs some pre-defined functions\n",
    "# both before and after running net.forward()\n",
    "# see http://pytorch.org/docs/master/_modules/torch/nn/modules/module.html\n",
    "\n",
    "print('input')\n",
    "print(X)\n",
    "\n",
    "print('\\noutput')\n",
    "print(net(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Parameter`s are a special kind of `Tensor`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's take a look at the gradients\n",
    "for p in net.parameters():\n",
    "    print(p.data)\n",
    "    print(p.grad)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.randn(7, num_features)\n",
    "out = net(X)\n",
    "# we need to give a tensor of gradients to .backward,\n",
    "# we give a dummy tensor\n",
    "out.backward(torch.randn(7, num_output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for details on `.backward()`, see http://pytorch.org/docs/master/autograd.html#torch.autograd.backward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's take a look at the gradients\n",
    "for p in net.parameters():\n",
    "    print(p.data)\n",
    "    print(p.grad)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ok, let's try and zero the accumulated gradients\n",
    "net.zero_grad()\n",
    "for p in net.parameters():\n",
    "    print(p.data)\n",
    "    print(p.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss function\n",
    "\n",
    "Let's define a custom loss function to compute how good our graph is doing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy(ys, ts):\n",
    "    # computing cross entropy per sample\n",
    "    cross_entropy = -torch.sum(ts * torch.log(ys), dim=1, keepdim=False)\n",
    "    # averaging over samples\n",
    "    return torch.mean(cross_entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train our neural network we need to update the parameters in the direction of the negative gradient w.r.t the cost function we defined earlier.\n",
    "We can use [`torch.optim`](http://pytorch.org/docs/master/optim.html) to get the gradients with some update rule for all parameters in the network.\n",
    "\n",
    "Heres a small animation of gradient descent: http://imgur.com/a/Hqolp, which also illustrates which challenges optimizers might face, e.g. saddle points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# Try Adam (typically faster to converge) or SGD with momentum\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.01)\n",
    "# Alternative:\n",
    "# optimizer = optim.SGD(net.parameters(), lr=0.01, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we make the prediction functions, such that we can get an accuracy measure over a batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(ys, ts):\n",
    "    # making a one-hot encoded vector of correct (1) and incorrect (0) predictions\n",
    "    correct_prediction = torch.eq(torch.max(ys, 1)[1], torch.max(ts, 1)[1])\n",
    "    # averaging the one-hot encoded vector\n",
    "    return torch.mean(correct_prediction.float())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to utilize our `optimizer` repeatedly in order to optimize our weights `W_1` and `b_1` to make the best possible linear seperation of the half moon dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of training passses\n",
    "num_epochs = 1000\n",
    "# store loss and accuracy for information\n",
    "train_losses, val_losses, train_accs, val_accs = [], [], [], []\n",
    "\n",
    "def pred(X):\n",
    "    \"\"\" Compute graph's prediction and return numpy array\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : numpy.ndarray\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    numpy.ndarray\n",
    "    \"\"\"\n",
    "    X = torch.from_numpy(X)\n",
    "    y = net(X)\n",
    "    return y.data.numpy()\n",
    "\n",
    "# plot boundary on testset before training session\n",
    "plot_decision_boundary(lambda x: pred(x), X_te, y_te)\n",
    "plt.title(\"Untrained Classifier\")\n",
    "\n",
    "# training loop\n",
    "for e in range(num_epochs):\n",
    "    # get training input and expected output as torch Variables and make sure type is correct\n",
    "    tr_input = torch.from_numpy(X_tr)\n",
    "    tr_targets = torch.from_numpy(onehot(y_tr, num_output)).float()\n",
    "    \n",
    "    # zeroize accumulated gradients in parameters\n",
    "    optimizer.zero_grad()\n",
    "    # predict by running forward pass\n",
    "    tr_output = net(tr_input)\n",
    "    # compute cross entropy loss\n",
    "    tr_loss = cross_entropy(tr_output, tr_targets)\n",
    "    # compute gradients given loss\n",
    "    tr_loss.backward()\n",
    "    # update the parameters given the computed gradients\n",
    "    optimizer.step()\n",
    "    train_acc = accuracy(tr_output, tr_targets)\n",
    "    \n",
    "    # store training loss\n",
    "    train_losses.append(tr_loss.data.numpy())\n",
    "    train_accs.append(train_acc)\n",
    "    \n",
    "    # get validation input and expected output as torch Variables and make sure type is correct\n",
    "    val_input = torch.from_numpy(X_val)\n",
    "    val_targets = torch.from_numpy(onehot(y_val, num_output)).float()\n",
    "    \n",
    "    # predict with validation input\n",
    "    val_output = net(val_input)\n",
    "    # compute loss and accuracy\n",
    "    val_loss = cross_entropy(val_output, val_targets)\n",
    "    val_acc = accuracy(val_output, val_targets)\n",
    "    \n",
    "    # store loss and accuracy\n",
    "    val_losses.append(val_loss.data.numpy())\n",
    "    val_accs.append(val_acc.data.numpy())\n",
    "    \n",
    "    if e % 100 == 0:\n",
    "        print(\"Epoch %i, \"\n",
    "              \"Train Cost: %0.3f\"\n",
    "              \"\\tVal Cost: %0.3f\"\n",
    "              \"\\t Val acc: %0.3f\" % (e, \n",
    "                                     train_losses[-1],\n",
    "                                     val_losses[-1],\n",
    "                                     val_accs[-1]))\n",
    "        \n",
    "        \n",
    "# get test input and expected output\n",
    "te_input = torch.from_numpy(X_te)\n",
    "te_targets = torch.from_numpy(onehot(y_te, num_output)).float()\n",
    "# predict on testset\n",
    "te_output = net(te_input)\n",
    "# compute loss and accuracy\n",
    "te_loss = cross_entropy(te_output, te_targets)\n",
    "te_acc = accuracy(te_output, te_targets)\n",
    "print(\"\\nTest Cost: %0.3f\\tTest Accuracy: %0.3f\" % (te_loss.data.numpy(), te_acc.data.numpy()))\n",
    "\n",
    "# plot boundary on testset after training session\n",
    "plot_decision_boundary(lambda x: pred(x), X_te, y_te)\n",
    "plt.title(\"Trained Classifier\")\n",
    "\n",
    "plt.figure()\n",
    "epoch = np.arange(len(train_losses))\n",
    "plt.plot(epoch, train_losses, 'r', label='Train Loss')\n",
    "plt.plot(epoch, val_losses, 'b', label='Val Loss')\n",
    "plt.legend()\n",
    "plt.xlabel('Updates')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(epoch, train_accs, 'r', label='Train Acc')\n",
    "plt.plot(epoch, val_accs, 'b', label='Val Acc')\n",
    "plt.legend()\n",
    "plt.xlabel('Updates')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignments\n",
    "\n",
    "1. A linear logistic classifier is only able to create a linear decision boundary. Change the Logistic classifier into a (nonlinear) Neural network by inserting a dense hidden layer between the input and output layers of the model\n",
    " \n",
    "2. Experiment with multiple hidden layers or more / less hidden units. What happens to the decision boundary?\n",
    " \n",
    "3. Overfitting: When increasing the number of hidden layers / units, the neural network will fit the training data better by creating a highly nonlinear decision boundary. If the model is too complex it will often generalize poorly to new data (validation and test set). Can you observe this from the training and validation errors? \n",
    " \n",
    "4. We used the vanilla stochastic gradient descent algorithm for parameter updates. This usually converges slowly and more sophisticated pseudo-second-order methods usually work better. Try changing the optimizer to [adam or momentum](http://pytorch.org/docs/master/optim.html#torch.optim.Adam)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution: Assignment 1 - Adding a Hidden Layer\n",
    "\n",
    "The `Net` class has been modified to include a hidden layer with 50 units and ReLU activation. The architecture is now:\n",
    "- **Input layer**: 2 features (x, y coordinates)\n",
    "- **Hidden layer**: 50 units with ReLU activation\n",
    "- **Output layer**: 2 units (binary classification) with softmax\n",
    "\n",
    "This allows the network to learn nonlinear decision boundaries, unlike the original logistic regression which could only separate classes linearly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution: Assignment 2 - Experimenting with Network Architecture\n",
    "\n",
    "Let's experiment with different hidden layer sizes to see how they affect the decision boundary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment with different architectures\n",
    "architectures = [\n",
    "    {\"name\": \"Small (10 units)\", \"hidden_units\": 10},\n",
    "    {\"name\": \"Medium (50 units)\", \"hidden_units\": 50},\n",
    "    {\"name\": \"Large (200 units)\", \"hidden_units\": 200},\n",
    "]\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "for idx, arch in enumerate(architectures):\n",
    "    # Create and train a new network\n",
    "    net_exp = Net(hidden_units=arch[\"hidden_units\"], activation=\"relu\")\n",
    "    optimizer_exp = optim.Adam(net_exp.parameters(), lr=0.01)\n",
    "    \n",
    "    # Quick training (fewer epochs for visualization)\n",
    "    for e in range(500):\n",
    "        tr_input = torch.from_numpy(X_tr)\n",
    "        tr_targets = torch.from_numpy(onehot(y_tr, num_output)).float()\n",
    "        \n",
    "        optimizer_exp.zero_grad()\n",
    "        tr_output = net_exp(tr_input)\n",
    "        tr_loss = cross_entropy(tr_output, tr_targets)\n",
    "        tr_loss.backward()\n",
    "        optimizer_exp.step()\n",
    "    \n",
    "    # Plot decision boundary\n",
    "    def pred_exp(X):\n",
    "        X_t = torch.from_numpy(X)\n",
    "        y = net_exp(X_t)\n",
    "        return y.data.numpy()\n",
    "    \n",
    "    x_min, x_max = X_te[:, 0].min() - .5, X_te[:, 0].max() + .5\n",
    "    y_min, y_max = X_te[:, 1].min() - .5, X_te[:, 1].max() + .5\n",
    "    h = 0.01\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "    yy = yy.astype('float32')\n",
    "    xx = xx.astype('float32')\n",
    "    Z = pred_exp(np.c_[xx.ravel(), yy.ravel()])[:,0]\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    \n",
    "    axes[idx].contourf(xx, yy, Z, cmap=plt.cm.RdBu, alpha=0.8)\n",
    "    axes[idx].scatter(X_te[:, 0], X_te[:, 1], c=-y_te, cmap=plt.cm.Spectral, edgecolors='k')\n",
    "    axes[idx].set_title(f\"{arch['name']}\")\n",
    "    axes[idx].set_xlabel('Feature 1')\n",
    "    axes[idx].set_ylabel('Feature 2')\n",
    "\n",
    "plt.suptitle('Effect of Hidden Layer Size on Decision Boundary', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nObservations:\")\n",
    "print(\"- Small networks (10 units): smoother, simpler decision boundaries\")\n",
    "print(\"- Medium networks (50 units): good balance of flexibility and generalization\")\n",
    "print(\"- Large networks (200 units): highly nonlinear, may fit training data very closely\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution: Assignment 3 - Observing Overfitting\n",
    "\n",
    "Now let's train a very large network (300 hidden units) and observe how it overfits to the training data by comparing training and validation losses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train an overly complex network\n",
    "net_overfit = Net(hidden_units=300, activation=\"relu\")\n",
    "optimizer_overfit = optim.Adam(net_overfit.parameters(), lr=0.01)\n",
    "\n",
    "num_epochs_overfit = 2000\n",
    "train_losses_overfit, val_losses_overfit = [], []\n",
    "train_accs_overfit, val_accs_overfit = [], []\n",
    "\n",
    "for e in range(num_epochs_overfit):\n",
    "    # Training\n",
    "    tr_input = torch.from_numpy(X_tr)\n",
    "    tr_targets = torch.from_numpy(onehot(y_tr, num_output)).float()\n",
    "    \n",
    "    optimizer_overfit.zero_grad()\n",
    "    tr_output = net_overfit(tr_input)\n",
    "    tr_loss = cross_entropy(tr_output, tr_targets)\n",
    "    tr_loss.backward()\n",
    "    optimizer_overfit.step()\n",
    "    train_acc = accuracy(tr_output, tr_targets)\n",
    "    \n",
    "    train_losses_overfit.append(tr_loss.data.numpy())\n",
    "    train_accs_overfit.append(train_acc.data.numpy())\n",
    "    \n",
    "    # Validation\n",
    "    val_input = torch.from_numpy(X_val)\n",
    "    val_targets = torch.from_numpy(onehot(y_val, num_output)).float()\n",
    "    val_output = net_overfit(val_input)\n",
    "    val_loss = cross_entropy(val_output, val_targets)\n",
    "    val_acc = accuracy(val_output, val_targets)\n",
    "    \n",
    "    val_losses_overfit.append(val_loss.data.numpy())\n",
    "    val_accs_overfit.append(val_acc.data.numpy())\n",
    "\n",
    "# Plot training vs validation loss\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "epoch = np.arange(len(train_losses_overfit))\n",
    "ax1.plot(epoch, train_losses_overfit, 'r', label='Train Loss', linewidth=2)\n",
    "ax1.plot(epoch, val_losses_overfit, 'b', label='Val Loss', linewidth=2)\n",
    "ax1.legend()\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.set_title('Training vs Validation Loss (300 hidden units)')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "ax2.plot(epoch, train_accs_overfit, 'r', label='Train Acc', linewidth=2)\n",
    "ax2.plot(epoch, val_accs_overfit, 'b', label='Val Acc', linewidth=2)\n",
    "ax2.legend()\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Accuracy')\n",
    "ax2.set_title('Training vs Validation Accuracy (300 hidden units)')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nFinal Training Loss: {train_losses_overfit[-1]:.4f}, Training Acc: {train_accs_overfit[-1]:.4f}\")\n",
    "print(f\"Final Validation Loss: {val_losses_overfit[-1]:.4f}, Validation Acc: {val_accs_overfit[-1]:.4f}\")\n",
    "print(\"\\nObservations:\")\n",
    "print(\"- If validation loss increases while training loss decreases, the model is overfitting\")\n",
    "print(\"- The gap between training and validation accuracy indicates overfitting\")\n",
    "print(\"- Complex models (300 units) can memorize training data but fail to generalize\")\n",
    "print(\"- The decision boundary becomes highly irregular and fits noise in the training data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution: Assignment 4 - Using Advanced Optimizers\n",
    "\n",
    "The optimizer has been changed from vanilla SGD to **Adam** (Adaptive Moment Estimation), which typically converges faster than SGD. Adam combines:\n",
    "- **Momentum**: uses exponentially weighted averages of past gradients\n",
    "- **RMSprop**: adapts learning rates for each parameter based on recent gradient magnitudes\n",
    "\n",
    "Alternative optimizers you can try:\n",
    "- `optim.SGD(net.parameters(), lr=0.01, momentum=0.9)` - SGD with momentum\n",
    "- `optim.RMSprop(net.parameters(), lr=0.01)` - RMSprop\n",
    "- `optim.AdamW(net.parameters(), lr=0.01)` - Adam with weight decay regularization\n",
    "\n",
    "Adam generally requires less hyperparameter tuning and converges faster than vanilla SGD, making it a popular choice for many deep learning tasks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
